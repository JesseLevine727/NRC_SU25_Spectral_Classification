{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff8f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa951c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam, p, niter = 1e4, 0.01, 10\n",
    "def baseline_als(y):\n",
    "    L = len(y)\n",
    "    D = np.diff(np.eye(L), 2)\n",
    "    D = lam * D.dot(D.T)\n",
    "    w = np.ones(L)\n",
    "    for _ in range(niter):\n",
    "        b = np.linalg.solve(np.diag(w) + D, w * y)\n",
    "        w = p * (y > b) + (1 - p) * (y < b)\n",
    "    return b\n",
    "\n",
    "def preprocess(arr):\n",
    "    out = np.zeros_like(arr)\n",
    "    for i, s in enumerate(arr):\n",
    "        b = baseline_als(s)\n",
    "        c = s - b\n",
    "        norm = np.linalg.norm(c)\n",
    "        out[i] = c / norm if norm > 0 else c\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a47bea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def floatify_cols(df):\n",
    "    new = []\n",
    "    for c in df.columns:\n",
    "        if c in ('Label', 'Label 1', 'Label 2'):\n",
    "            new.append(c)\n",
    "        else:\n",
    "            new.append(float(c))\n",
    "    df.columns = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df = pd.read_csv('reference_v2.csv')\n",
    "\n",
    "def floatify_cols(df):\n",
    "    new = []\n",
    "    for c in df.columns:\n",
    "        if c in ('Label', 'Label 1', 'Label 2'):\n",
    "            new.append(c)\n",
    "        else:\n",
    "            new.append(float(c))\n",
    "    df.columns = new\n",
    "floatify_cols(ref_df)\n",
    "\n",
    "wav_cols   = [c for c in ref_df.columns if c!='Label']\n",
    "ref_specs  = ref_df[wav_cols].values\n",
    "ref_labels = ref_df['Label'].values\n",
    "ref_proc   = preprocess(ref_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aadf01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic raw spectra: (12540, 1024)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "# Unique chemical classes\n",
    "classes    = sorted(np.unique(ref_labels))\n",
    "C = len(classes)\n",
    "class_to_i = {c:i for i,c in enumerate(classes)}\n",
    "\n",
    "# ─── 2) Generate synthetic mixtures ────────────────────────────────────────────\n",
    "ratios = np.arange(0.05, 1.0, 0.05)\n",
    "noise_level = 0.01\n",
    "n_per_ratio = 10  # number of random spectra per pair/ratio\n",
    "\n",
    "synth_specs = []\n",
    "synth_labels = []\n",
    "for (i, ci), (j, cj) in combinations(enumerate(classes), 2):\n",
    "    # indices of pure spectra for each class\n",
    "    idx_i = np.where(ref_labels == ci)[0]\n",
    "    idx_j = np.where(ref_labels == cj)[0]\n",
    "    for r in ratios:\n",
    "        for _ in range(n_per_ratio):\n",
    "            spec_i = ref_specs[np.random.choice(idx_i)]\n",
    "            spec_j = ref_specs[np.random.choice(idx_j)]\n",
    "            mix = r * spec_i + (1-r) * spec_j\n",
    "            mix += np.random.normal(scale=noise_level, size=mix.shape)\n",
    "            synth_specs.append(mix)\n",
    "            synth_labels.append((ci, cj))\n",
    "synth_specs = np.array(synth_specs)        # (n_synth, n_waves)\n",
    "print(\"Synthetic raw spectra:\", synth_specs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c34d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 153 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 253 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 309 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 338 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done 369 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 400 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done 433 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done 466 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done 501 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done 536 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 573 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done 610 tasks      | elapsed:   20.4s\n",
      "[Parallel(n_jobs=-1)]: Done 649 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 688 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done 770 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done 813 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-1)]: Done 856 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done 901 tasks      | elapsed:   26.9s\n",
      "[Parallel(n_jobs=-1)]: Done 946 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=-1)]: Done 993 tasks      | elapsed:   29.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1040 tasks      | elapsed:   30.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1089 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1138 tasks      | elapsed:   32.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1189 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1240 tasks      | elapsed:   34.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1293 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1346 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1401 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1456 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1513 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1570 tasks      | elapsed:   42.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1629 tasks      | elapsed:   43.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1688 tasks      | elapsed:   44.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1749 tasks      | elapsed:   46.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1810 tasks      | elapsed:   47.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1873 tasks      | elapsed:   48.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1936 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2001 tasks      | elapsed:   51.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2066 tasks      | elapsed:   53.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2133 tasks      | elapsed:   54.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2200 tasks      | elapsed:   56.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2269 tasks      | elapsed:   57.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2338 tasks      | elapsed:   59.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2409 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2480 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2553 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2626 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2701 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2776 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2853 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2930 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3009 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3088 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3169 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3250 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3333 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3416 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3501 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3586 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3673 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3760 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3849 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3938 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4029 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4120 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4213 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4306 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4401 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4496 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4593 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4690 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4789 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4888 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4989 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5090 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5193 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5296 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5401 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5506 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5613 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5720 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5829 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5938 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6049 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6160 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6273 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6386 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6501 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6616 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6733 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6850 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6969 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 7088 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7209 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7330 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7453 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7576 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7701 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7826 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7953 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 8080 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 8209 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8338 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8469 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8600 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8733 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8866 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 9001 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9136 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9273 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9410 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9549 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9688 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9829 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9970 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10113 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10256 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10401 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10546 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10693 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10840 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10989 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 11138 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11289 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11440 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11593 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11746 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11901 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12056 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12213 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12370 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel preprocess done: (12540, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 12540 out of 12540 | elapsed:  4.8min finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def preprocess_single(spectrum):\n",
    "    \"\"\"\n",
    "    Baseline‐correct + L2‐normalize + abs, for one 1D array.\n",
    "    \"\"\"\n",
    "    b = baseline_als(spectrum)\n",
    "    c = spectrum - b\n",
    "    norm = np.linalg.norm(c)\n",
    "    out = c / norm if norm > 0 else c\n",
    "    return np.abs(out)\n",
    "\n",
    "# 2) Parallel map over all spectra\n",
    "#    n_jobs=-1 uses all CPUs; you can set e.g. n_jobs=4 to use 4 cores.\n",
    "synth_proc = np.vstack(\n",
    "    Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(preprocess_single)(spec) \n",
    "        for spec in synth_specs\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Parallel preprocess done:\", synth_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "708c5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 4) Split synthetic into train/test for Siamese & evaluation ──────────────\n",
    "X_train, X_test, y_train_pairs, y_test_pairs = train_test_split(\n",
    "    synth_proc, synth_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd780f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 5) Dataset for contrastive learning (mixture class as label) ─────────────\n",
    "class RamanPairDataset(Dataset):\n",
    "    def __init__(self, specs, pair_labels, augment_fn=None):\n",
    "        self.specs  = specs\n",
    "        self.labels = pair_labels\n",
    "        self.augment = augment_fn\n",
    "        # index by mixture class\n",
    "        self.by_label = {}\n",
    "        for idx, lab in enumerate(pair_labels):\n",
    "            self.by_label.setdefault(lab, []).append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.specs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns one contrastive pair for the mixture at index `idx`:\n",
    "        - x1: the mixture spectrum\n",
    "        - x2: either a positive (same mixture class) or negative (different mixture class) spectrum\n",
    "        - y: 1.0 if positive pair, 0.0 if negative pair\n",
    "        \"\"\"\n",
    "        import random\n",
    "\n",
    "        # Anchor: the mixture spectrum\n",
    "        x1 = self.specs[idx]\n",
    "        lab1 = self.labels[idx]\n",
    "\n",
    "        # Randomly decide positive vs. negative pair\n",
    "        if random.random() < 0.5:\n",
    "            # Positive pair: pick another instance of the same mixture class\n",
    "            pos_idx = random.choice(self.by_label[lab1])\n",
    "            x2 = self.specs[pos_idx]\n",
    "            y = 1.0\n",
    "        else:\n",
    "            # Negative pair: pick a different mixture class\n",
    "            neg_classes = [l for l in self.by_label.keys() if l != lab1]\n",
    "            neg_lab = random.choice(neg_classes)\n",
    "            neg_idx = random.choice(self.by_label[neg_lab])\n",
    "            x2 = self.specs[neg_idx]\n",
    "            y = 0.0\n",
    "\n",
    "        # Apply augmentation if provided\n",
    "        if self.augment:\n",
    "            x1 = self.augment(x1)\n",
    "            x2 = self.augment(x2)\n",
    "\n",
    "        # Return tensors ready for the Siamese network\n",
    "        return (\n",
    "            torch.tensor(x1, dtype=torch.float32).unsqueeze(0),\n",
    "            torch.tensor(x2, dtype=torch.float32).unsqueeze(0),\n",
    "            torch.tensor(y, dtype=torch.float32)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "194f44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(spec, noise_std=0.01, shift_max=2):\n",
    "    spec_noisy = spec + np.random.normal(0, noise_std, size=spec.shape)\n",
    "    shift = np.random.randint(-shift_max, shift_max + 1)\n",
    "    return np.roll(spec_noisy, shift)\n",
    "train_ds = RamanPairDataset(X_train, y_train_pairs, augment_fn=augment)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2716a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6) Siamese network & contrastive loss ────────────────────────────────────\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, input_len, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1,16,7,padding=3), nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(16,32,5,padding=2), nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_len//4)*32, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        return F.normalize(z, dim=1)\n",
    "\n",
    "def contrastive_loss(z1, z2, label, margin=1.0):\n",
    "    dist = F.pairwise_distance(z1, z2)\n",
    "    return (label*dist**2 + (1-label)*F.relu(margin-dist)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb3b568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6) Train the Siamese ──────────────────────────────────────────────────────\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseNet(input_len=ref_proc.shape[1], embed_dim=64).to(device)\n",
    "opt     = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fde63d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 Loss: 0.0517\n",
      "Epoch 002 Loss: 0.0503\n",
      "Epoch 003 Loss: 0.0499\n",
      "Epoch 004 Loss: 0.0496\n",
      "Epoch 005 Loss: 0.0509\n",
      "Epoch 006 Loss: 0.0525\n",
      "Epoch 007 Loss: 0.0502\n",
      "Epoch 008 Loss: 0.0497\n",
      "Epoch 009 Loss: 0.0502\n",
      "Epoch 010 Loss: 0.0519\n"
     ]
    }
   ],
   "source": [
    "# ─── 7) Train Siamese on synthetic mixtures ───────────────────────────────────\n",
    "for epoch in range(1, 11):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for x1, x2, l in train_loader:\n",
    "        x1,x2,l = x1.to(device), x2.to(device), l.to(device)\n",
    "        z1, z2 = model(x1), model(x2)\n",
    "        loss = contrastive_loss(z1, z2, l)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        total += loss.item()*x1.size(0)\n",
    "    print(f\"Epoch {epoch:03d} Loss: {total/len(train_ds):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea61ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"siamese_mixture.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2667d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 8) Embed & 1-NN classify synthetic test ─────────────────────────────────\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_emb = model(torch.tensor(X_train, dtype=torch.float32).unsqueeze(1).to(device)).cpu().numpy()\n",
    "    test_emb  = model(torch.tensor(X_test,  dtype=torch.float32).unsqueeze(1).to(device)).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff8ade5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synthetic Test Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "           1,9-nonanedithiol       0.97      0.97      0.97       425\n",
      "             1-dodecanethiol       0.91      0.89      0.90       441\n",
      "             1-undecanethiol       0.88      0.91      0.89       400\n",
      "        6-mercapto-1-hexanol       0.97      0.97      0.97       431\n",
      "                     benzene       1.00      1.00      1.00       409\n",
      "                benzenethiol       0.99      0.99      0.99       396\n",
      "                        dmmp       1.00      1.00      1.00       439\n",
      "                        etoh       0.99      1.00      0.99       441\n",
      "                        meoh       0.99      1.00      0.99       399\n",
      "       n,n-dimethylformamide       1.00      1.00      1.00       415\n",
      "                    pyridine       1.00      1.00      1.00       413\n",
      "tris(2-ethylhexyl) phosphate       0.99      0.98      0.99       407\n",
      "\n",
      "                   micro avg       0.97      0.97      0.97      5016\n",
      "                   macro avg       0.98      0.98      0.98      5016\n",
      "                weighted avg       0.98      0.97      0.97      5016\n",
      "                 samples avg       0.97      0.97      0.97      5016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build true multi-hot for synthetic test\n",
    "C = len(classes)\n",
    "# Build true multi-hot for synthetic test\n",
    "Y_true_s = np.zeros((len(X_test), C), dtype=int)\n",
    "for i, pair in enumerate(y_test_pairs):\n",
    "    if isinstance(pair, str):\n",
    "        c1, c2 = pair.split(\"__\")\n",
    "    else:\n",
    "        c1, c2 = pair  # already a tuple\n",
    "    Y_true_s[i, class_to_i[c1]] = 1\n",
    "    Y_true_s[i, class_to_i[c2]] = 1\n",
    "\n",
    "\n",
    "# NN classification\n",
    "# 1-NN classification of synthetic test\n",
    "Y_pred_s = np.zeros_like(Y_true_s)\n",
    "for i, z in enumerate(test_emb):\n",
    "    idx = np.argmin(np.linalg.norm(train_emb - z, axis=1))\n",
    "    pred_pair = y_train_pairs[idx]\n",
    "    \n",
    "    # unpack regardless of tuple or string\n",
    "    if isinstance(pred_pair, str):\n",
    "        c1, c2 = pred_pair.split(\"__\")\n",
    "    else:\n",
    "        c1, c2 = pred_pair\n",
    "\n",
    "    Y_pred_s[i, class_to_i[c1]] = 1\n",
    "    Y_pred_s[i, class_to_i[c2]] = 1\n",
    "\n",
    "\n",
    "print(\"\\nSynthetic Test Report:\")\n",
    "print(classification_report(Y_true_s, Y_pred_s, target_names=classes, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9623fadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 338 tasks      | elapsed:   14.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixtures preprocessed: (580, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 580 out of 580 | elapsed:   19.1s finished\n"
     ]
    }
   ],
   "source": [
    "# --- Load mixtures and convert columns ---\n",
    "mix_df = pd.read_csv('mixtures_dataset.csv')\n",
    "\n",
    "# Re-use your floatify_cols helper:\n",
    "def floatify_cols(df):\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        if c in ('Label 1', 'Label 2'):\n",
    "            new_cols.append(c)\n",
    "        else:\n",
    "            new_cols.append(float(c))  # convert wavenumber strings → floats\n",
    "    df.columns = new_cols\n",
    "\n",
    "floatify_cols(mix_df)\n",
    "\n",
    "# --- Now select wavenumber columns (they are all numeric) ---\n",
    "wav_cols = [c for c in mix_df.columns if c not in ('Label 1', 'Label 2')]\n",
    "\n",
    "# --- Extract spectra as a pure float array ---\n",
    "mix_specs = mix_df[wav_cols].values.astype(float)  # ensure float64 dtype\n",
    "\n",
    "# --- Parallel preprocessing now works because mix_specs is numeric ---\n",
    "mix_proc = np.vstack(\n",
    "    Parallel(n_jobs=-1, verbose=5)(\n",
    "        delayed(preprocess_single)(spec) for spec in mix_specs\n",
    "    )\n",
    ")\n",
    "print(\"Mixtures preprocessed:\", mix_proc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b87515bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    real_emb = model(torch.tensor(mix_proc, dtype=torch.float32).unsqueeze(1).to(device)).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b14d430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "      1-dodecanethiol       0.89      0.54      0.67       243\n",
      " 6-mercapto-1-hexanol       0.99      0.76      0.86       108\n",
      "              benzene       1.00      1.00      1.00       193\n",
      "         benzenethiol       0.50      0.50      0.50        72\n",
      "                 etoh       0.70      0.70      0.70       121\n",
      "                 meoh       1.00      0.97      0.98       243\n",
      "n,n-dimethylformamide       1.00      1.00      1.00        72\n",
      "             pyridine       1.00      1.00      1.00       108\n",
      "\n",
      "            micro avg       0.91      0.81      0.86      1160\n",
      "            macro avg       0.88      0.81      0.84      1160\n",
      "         weighted avg       0.91      0.81      0.85      1160\n",
      "          samples avg       0.91      0.81      0.85      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# true real multi-hot\n",
    "Y_true_r = np.zeros((len(mix_df), C), int)\n",
    "for i,(l1,l2) in enumerate(zip(mix_df['Label 1'], mix_df['Label 2'])):\n",
    "    Y_true_r[i, class_to_i[l1]] = 1\n",
    "    Y_true_r[i, class_to_i[l2]] = 1\n",
    "\n",
    "# classify real\n",
    "# 1-NN classification of real mixtures\n",
    "Y_pred_r = np.zeros_like(Y_true_r)\n",
    "for i, z in enumerate(real_emb):\n",
    "    idx = np.argmin(np.linalg.norm(train_emb - z, axis=1))\n",
    "    pred_pair = y_train_pairs[idx]\n",
    "    if isinstance(pred_pair, str):\n",
    "        c1, c2 = pred_pair.split(\"__\")\n",
    "    else:\n",
    "        c1, c2 = pred_pair\n",
    "\n",
    "    Y_pred_r[i, class_to_i[c1]] = 1\n",
    "    Y_pred_r[i, class_to_i[c2]] = 1\n",
    "\n",
    "\n",
    "# Assuming you’ve already built Y_true_r, Y_pred_r, and classes previously:\n",
    "support = Y_true_r.sum(axis=0)\n",
    "valid_idxs = np.where(support > 0)[0]\n",
    "valid_names = [classes[i] for i in valid_idxs]\n",
    "\n",
    "Y_true_filtered = Y_true_r[:, valid_idxs]\n",
    "Y_pred_filtered = Y_pred_r[:, valid_idxs]\n",
    "\n",
    "print(classification_report(\n",
    "    Y_true_filtered,\n",
    "    Y_pred_filtered,\n",
    "    target_names=valid_names,\n",
    "    zero_division=0\n",
    "))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

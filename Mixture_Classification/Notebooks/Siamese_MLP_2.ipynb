{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190ee5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2230f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Utilities ────────────────────────────────────────────────────────────────\n",
    "lam, p, niter = 1e4, 0.01, 10\n",
    "def baseline_als(y):\n",
    "    L = len(y)\n",
    "    D = np.diff(np.eye(L), 2)\n",
    "    D = lam * D.dot(D.T)\n",
    "    w = np.ones(L)\n",
    "    for _ in range(niter):\n",
    "        b = np.linalg.solve(np.diag(w) + D, w * y)\n",
    "        w = p * (y > b) + (1 - p) * (y < b)\n",
    "    return b\n",
    "\n",
    "def preprocess(arr):\n",
    "    out = np.zeros_like(arr)\n",
    "    for i, s in enumerate(arr):\n",
    "        b = baseline_als(s)\n",
    "        c = s - b\n",
    "        norm = np.linalg.norm(c)\n",
    "        out[i] = c / norm if norm > 0 else c\n",
    "    return out\n",
    "\n",
    "def floatify_cols(df):\n",
    "    new = []\n",
    "    for c in df.columns:\n",
    "        if c in ('Label', 'Label 1', 'Label 2'):\n",
    "            new.append(c)\n",
    "        else:\n",
    "            new.append(float(c))\n",
    "    df.columns = new\n",
    "\n",
    "# --- Augmentation Functions ---\n",
    "def add_baseline_drift(spec, degree=3, scale=0.1):\n",
    "    \"\"\"Add a random polynomial baseline of given degree.\"\"\"\n",
    "    n = len(spec)\n",
    "    x = np.linspace(0, 1, n)\n",
    "    coeffs = np.random.randn(degree + 1) * scale\n",
    "    baseline = np.polyval(coeffs, x)\n",
    "    return spec + baseline\n",
    "\n",
    "def jitter_wavenumber(spec, max_shift=2):\n",
    "    \"\"\"Apply a small horizontal shift to simulate calibration jitter.\"\"\"\n",
    "    n = len(spec)\n",
    "    x = np.arange(n)\n",
    "    shift = np.random.uniform(-max_shift, max_shift)\n",
    "    f = interp1d(x, spec, kind='cubic', fill_value=\"extrapolate\")\n",
    "    return f(x + shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b380292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) Load reference_v2 and preprocess ──────────────────────────────────────\n",
    "ref_df = pd.read_csv('reference_v2.csv')\n",
    "\n",
    "floatify_cols(ref_df)\n",
    "wav_cols = [c for c in ref_df.columns if c != 'Label']\n",
    "ref_specs  = ref_df[wav_cols].values       # (n_ref_samples, n_waves)\n",
    "ref_labels = ref_df['Label'].values        # (n_ref_samples,)\n",
    "ref_proc   = preprocess(ref_specs)         # (n_ref_samples, n_waves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22eea9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic raw spectra after oversampling & augmentation: (16530, 1024)\n"
     ]
    }
   ],
   "source": [
    "# --- Refactored Synthetic Mixture Generation ---\n",
    "classes       = sorted(np.unique(ref_labels))\n",
    "class_to_i    = {c: i for i, c in enumerate(classes)}\n",
    "ratios        = np.arange(0.05, 1.0, 0.05)\n",
    "noise_level   = 0.01\n",
    "base_n        = 10  # base number of spectra per pair/ratio\n",
    "oversample_set = {\n",
    "    'meoh','6-mercapto-1-hexanol'\n",
    "}\n",
    "\n",
    "synth_specs  = []\n",
    "synth_labels = []\n",
    "\n",
    "for (ci, cj) in combinations(classes, 2):\n",
    "    idx_i = np.where(ref_labels == ci)[0]\n",
    "    idx_j = np.where(ref_labels == cj)[0]\n",
    "    # Determine how many per ratio\n",
    "    n_per_ratio = base_n * 2 if (ci in oversample_set or cj in oversample_set) else base_n\n",
    "    \n",
    "    for r in ratios:\n",
    "        for _ in range(n_per_ratio):\n",
    "            # pick random pure spectra\n",
    "            spec_i = ref_specs[np.random.choice(idx_i)]\n",
    "            spec_j = ref_specs[np.random.choice(idx_j)]\n",
    "            # linear mix\n",
    "            mix = r * spec_i + (1 - r) * spec_j\n",
    "            # augmentations\n",
    "            mix = add_baseline_drift(mix, degree=3, scale=0.1)\n",
    "            mix = jitter_wavenumber(mix, max_shift=2)\n",
    "            mix += np.random.normal(scale=noise_level, size=mix.shape)\n",
    "            # store\n",
    "            synth_specs.append(mix)\n",
    "            synth_labels.append((ci, cj))\n",
    "\n",
    "synth_specs = np.array(synth_specs)\n",
    "print(\"Synthetic raw spectra after oversampling & augmentation:\", synth_specs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6d06be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 131 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 202 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 227 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 254 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 310 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 339 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 370 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 401 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 467 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 502 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=-1)]: Done 537 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 574 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done 611 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done 650 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done 689 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done 730 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 771 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done 814 tasks      | elapsed:   24.6s\n",
      "[Parallel(n_jobs=-1)]: Done 857 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 902 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done 947 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1090 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1139 tasks      | elapsed:   32.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1190 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1241 tasks      | elapsed:   34.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1294 tasks      | elapsed:   35.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1347 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1402 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1457 tasks      | elapsed:   39.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1514 tasks      | elapsed:   40.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1571 tasks      | elapsed:   41.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1630 tasks      | elapsed:   43.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1689 tasks      | elapsed:   44.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1750 tasks      | elapsed:   45.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1811 tasks      | elapsed:   47.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1874 tasks      | elapsed:   48.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1937 tasks      | elapsed:   49.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2002 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2067 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2134 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2201 tasks      | elapsed:   55.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2270 tasks      | elapsed:   57.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2339 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2410 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2481 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2554 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2627 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2702 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2777 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2854 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2931 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3010 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3089 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3170 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3251 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3334 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3417 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3502 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3587 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3674 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3761 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3850 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3939 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4030 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4121 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4214 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4307 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4402 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4497 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4594 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4691 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4790 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4889 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4990 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5091 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5194 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5297 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5402 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5507 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5614 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5721 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5830 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5939 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6050 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6161 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6274 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6387 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6502 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6617 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6734 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6851 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6970 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 7089 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7210 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7331 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7454 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7577 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7702 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7827 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7954 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 8081 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8210 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8339 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8470 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8601 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8734 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8867 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9002 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9137 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9274 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9411 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9550 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9689 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9830 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9971 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10114 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10257 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10402 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10547 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10694 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10841 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10990 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11139 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11290 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11441 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11594 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11747 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 11902 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12057 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12214 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12371 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 12530 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 12689 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 12850 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 13011 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 13174 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 13337 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 13502 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 13667 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 13834 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 14001 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 14170 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 14339 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 14510 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 14681 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 14854 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 15027 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 15202 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 15377 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 15554 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 15731 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 15910 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 16089 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 16270 tasks      | elapsed:  6.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel preprocess done: (16530, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 16530 out of 16530 | elapsed:  6.4min finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def preprocess_single(spectrum):\n",
    "    \"\"\"\n",
    "    Baseline‐correct + L2‐normalize + abs, for one 1D array.\n",
    "    \"\"\"\n",
    "    b = baseline_als(spectrum)\n",
    "    c = spectrum - b\n",
    "    norm = np.linalg.norm(c)\n",
    "    out = c / norm if norm > 0 else c\n",
    "    return np.abs(out)\n",
    "\n",
    "# 2) Parallel map over all spectra\n",
    "#    n_jobs=-1 uses all CPUs; you can set e.g. n_jobs=4 to use 4 cores.\n",
    "synth_proc = np.vstack(\n",
    "    Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(preprocess_single)(spec) \n",
    "        for spec in synth_specs\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Parallel preprocess done:\", synth_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "229e9aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic embeddings: (16530, 64)\n"
     ]
    }
   ],
   "source": [
    "# ─── 4) Embed synthetic mixtures via Siamese ──────────────────────────────────\n",
    "# load siamese\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, input_len, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1,16,7,padding=3), nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(16,32,5,padding=2), nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_len//4)*32, embed_dim), nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        return F.normalize(z, dim=1)\n",
    "\n",
    "siamese = SiameseNet(input_len=ref_proc.shape[1], embed_dim=64)\n",
    "siamese.load_state_dict(torch.load('siamese_raman_resampled.pth', map_location='cpu'))\n",
    "siamese.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    tensor = torch.tensor(synth_proc, dtype=torch.float32).unsqueeze(1)\n",
    "    syn_embeds = siamese(tensor).cpu().numpy()  # (n_synth, 64)\n",
    "print(\"Synthetic embeddings:\", syn_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c4c2906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 5) Build X_synth, Y_synth ─────────────────────────────────────────────────\n",
    "C = len(classes)\n",
    "N = len(syn_embeds)\n",
    "X_synth = syn_embeds\n",
    "Y_synth = np.zeros((N, C), dtype=int)\n",
    "for k, (ci, cj) in enumerate(synth_labels):\n",
    "    Y_synth[k, class_to_i[ci]] = 1\n",
    "    Y_synth[k, class_to_i[cj]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5f1630c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic train/val/test: 13224 1653 1653\n"
     ]
    }
   ],
   "source": [
    "# ─── 6) Split synthetic into train/val/test (80/10/10) ─────────────────────────\n",
    "X_tmp, X_test_s, Y_tmp, Y_test_s = train_test_split(X_synth, Y_synth, test_size=0.10, random_state=0)\n",
    "X_train_s, X_val_s, Y_train_s, Y_val_s = train_test_split(X_tmp, Y_tmp, test_size=0.1111, random_state=0)\n",
    "print(\"Synthetic train/val/test:\", len(X_train_s), len(X_val_s), len(X_test_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f766e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 7) DataLoaders for synthetic ─────────────────────────────────────────────\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(X_train_s, dtype=torch.float32),\n",
    "        torch.tensor(Y_train_s, dtype=torch.float32)   # ← make this float\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(X_val_s, dtype=torch.float32),\n",
    "        torch.tensor(Y_val_s, dtype=torch.float32)     # ← and this\n",
    "    ),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(X_test_s, dtype=torch.float32),\n",
    "        torch.tensor(Y_test_s, dtype=torch.float32)    # ← and this\n",
    "    ),\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e8dc1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 — Train Loss: 0.4409 | Val Loss: 0.3367\n",
      "Epoch 2/200 — Train Loss: 0.3035 | Val Loss: 0.2857\n",
      "Epoch 3/200 — Train Loss: 0.2735 | Val Loss: 0.2687\n",
      "Epoch 4/200 — Train Loss: 0.2611 | Val Loss: 0.2590\n",
      "Epoch 5/200 — Train Loss: 0.2527 | Val Loss: 0.2520\n",
      "Epoch 6/200 — Train Loss: 0.2466 | Val Loss: 0.2464\n",
      "Epoch 7/200 — Train Loss: 0.2417 | Val Loss: 0.2422\n",
      "Epoch 8/200 — Train Loss: 0.2381 | Val Loss: 0.2401\n",
      "Epoch 9/200 — Train Loss: 0.2352 | Val Loss: 0.2349\n",
      "Epoch 10/200 — Train Loss: 0.2322 | Val Loss: 0.2321\n",
      "Epoch 11/200 — Train Loss: 0.2297 | Val Loss: 0.2310\n",
      "Epoch 12/200 — Train Loss: 0.2274 | Val Loss: 0.2277\n",
      "Epoch 13/200 — Train Loss: 0.2252 | Val Loss: 0.2253\n",
      "Epoch 14/200 — Train Loss: 0.2234 | Val Loss: 0.2228\n",
      "Epoch 15/200 — Train Loss: 0.2214 | Val Loss: 0.2207\n",
      "Epoch 16/200 — Train Loss: 0.2197 | Val Loss: 0.2199\n",
      "Epoch 17/200 — Train Loss: 0.2182 | Val Loss: 0.2174\n",
      "Epoch 18/200 — Train Loss: 0.2166 | Val Loss: 0.2162\n",
      "Epoch 19/200 — Train Loss: 0.2149 | Val Loss: 0.2148\n",
      "Epoch 20/200 — Train Loss: 0.2135 | Val Loss: 0.2131\n",
      "Epoch 21/200 — Train Loss: 0.2119 | Val Loss: 0.2111\n",
      "Epoch 22/200 — Train Loss: 0.2105 | Val Loss: 0.2106\n",
      "Epoch 23/200 — Train Loss: 0.2089 | Val Loss: 0.2087\n",
      "Epoch 24/200 — Train Loss: 0.2073 | Val Loss: 0.2082\n",
      "Epoch 25/200 — Train Loss: 0.2060 | Val Loss: 0.2067\n",
      "Epoch 26/200 — Train Loss: 0.2045 | Val Loss: 0.2043\n",
      "Epoch 27/200 — Train Loss: 0.2032 | Val Loss: 0.2031\n",
      "Epoch 28/200 — Train Loss: 0.2017 | Val Loss: 0.2029\n",
      "Epoch 29/200 — Train Loss: 0.2003 | Val Loss: 0.2004\n",
      "Epoch 30/200 — Train Loss: 0.1992 | Val Loss: 0.1992\n",
      "Epoch 31/200 — Train Loss: 0.1981 | Val Loss: 0.1976\n",
      "Epoch 32/200 — Train Loss: 0.1966 | Val Loss: 0.1968\n",
      "Epoch 33/200 — Train Loss: 0.1955 | Val Loss: 0.1956\n",
      "Epoch 34/200 — Train Loss: 0.1947 | Val Loss: 0.1942\n",
      "Epoch 35/200 — Train Loss: 0.1931 | Val Loss: 0.1940\n",
      "Epoch 36/200 — Train Loss: 0.1923 | Val Loss: 0.1917\n",
      "Epoch 37/200 — Train Loss: 0.1910 | Val Loss: 0.1917\n",
      "Epoch 38/200 — Train Loss: 0.1905 | Val Loss: 0.1907\n",
      "Epoch 39/200 — Train Loss: 0.1890 | Val Loss: 0.1891\n",
      "Epoch 40/200 — Train Loss: 0.1883 | Val Loss: 0.1884\n",
      "Epoch 41/200 — Train Loss: 0.1870 | Val Loss: 0.1874\n",
      "Epoch 42/200 — Train Loss: 0.1864 | Val Loss: 0.1854\n",
      "Epoch 43/200 — Train Loss: 0.1854 | Val Loss: 0.1854\n",
      "Epoch 44/200 — Train Loss: 0.1846 | Val Loss: 0.1854\n",
      "Epoch 45/200 — Train Loss: 0.1835 | Val Loss: 0.1846\n",
      "Epoch 46/200 — Train Loss: 0.1832 | Val Loss: 0.1827\n",
      "Epoch 47/200 — Train Loss: 0.1820 | Val Loss: 0.1823\n",
      "Epoch 48/200 — Train Loss: 0.1814 | Val Loss: 0.1822\n",
      "Epoch 49/200 — Train Loss: 0.1805 | Val Loss: 0.1807\n",
      "Epoch 50/200 — Train Loss: 0.1797 | Val Loss: 0.1792\n",
      "Epoch 51/200 — Train Loss: 0.1794 | Val Loss: 0.1793\n",
      "Epoch 52/200 — Train Loss: 0.1787 | Val Loss: 0.1802\n",
      "Epoch 53/200 — Train Loss: 0.1780 | Val Loss: 0.1771\n",
      "Epoch 54/200 — Train Loss: 0.1771 | Val Loss: 0.1771\n",
      "Epoch 55/200 — Train Loss: 0.1765 | Val Loss: 0.1759\n",
      "Epoch 56/200 — Train Loss: 0.1758 | Val Loss: 0.1764\n",
      "Epoch 57/200 — Train Loss: 0.1757 | Val Loss: 0.1762\n",
      "Epoch 58/200 — Train Loss: 0.1746 | Val Loss: 0.1749\n",
      "Epoch 59/200 — Train Loss: 0.1742 | Val Loss: 0.1746\n",
      "Epoch 60/200 — Train Loss: 0.1737 | Val Loss: 0.1734\n",
      "Epoch 61/200 — Train Loss: 0.1731 | Val Loss: 0.1734\n",
      "Epoch 62/200 — Train Loss: 0.1723 | Val Loss: 0.1727\n",
      "Epoch 63/200 — Train Loss: 0.1720 | Val Loss: 0.1720\n",
      "Epoch 64/200 — Train Loss: 0.1714 | Val Loss: 0.1719\n",
      "Epoch 65/200 — Train Loss: 0.1713 | Val Loss: 0.1711\n",
      "Epoch 66/200 — Train Loss: 0.1705 | Val Loss: 0.1696\n",
      "Epoch 67/200 — Train Loss: 0.1699 | Val Loss: 0.1690\n",
      "Epoch 68/200 — Train Loss: 0.1696 | Val Loss: 0.1701\n",
      "Epoch 69/200 — Train Loss: 0.1692 | Val Loss: 0.1685\n",
      "Epoch 70/200 — Train Loss: 0.1688 | Val Loss: 0.1677\n",
      "Epoch 71/200 — Train Loss: 0.1684 | Val Loss: 0.1690\n",
      "Epoch 72/200 — Train Loss: 0.1679 | Val Loss: 0.1673\n",
      "Epoch 73/200 — Train Loss: 0.1672 | Val Loss: 0.1677\n",
      "Epoch 74/200 — Train Loss: 0.1672 | Val Loss: 0.1660\n",
      "Epoch 75/200 — Train Loss: 0.1663 | Val Loss: 0.1677\n",
      "Epoch 76/200 — Train Loss: 0.1664 | Val Loss: 0.1651\n",
      "Epoch 77/200 — Train Loss: 0.1659 | Val Loss: 0.1666\n",
      "Epoch 78/200 — Train Loss: 0.1655 | Val Loss: 0.1651\n",
      "Epoch 79/200 — Train Loss: 0.1648 | Val Loss: 0.1644\n",
      "Epoch 80/200 — Train Loss: 0.1651 | Val Loss: 0.1671\n",
      "Epoch 81/200 — Train Loss: 0.1644 | Val Loss: 0.1644\n",
      "Epoch 82/200 — Train Loss: 0.1643 | Val Loss: 0.1636\n",
      "Epoch 83/200 — Train Loss: 0.1635 | Val Loss: 0.1637\n",
      "Epoch 84/200 — Train Loss: 0.1635 | Val Loss: 0.1632\n",
      "Epoch 85/200 — Train Loss: 0.1629 | Val Loss: 0.1624\n",
      "Epoch 86/200 — Train Loss: 0.1629 | Val Loss: 0.1633\n",
      "Epoch 87/200 — Train Loss: 0.1625 | Val Loss: 0.1634\n",
      "Epoch 88/200 — Train Loss: 0.1621 | Val Loss: 0.1626\n",
      "Epoch 89/200 — Train Loss: 0.1620 | Val Loss: 0.1622\n",
      "Epoch 90/200 — Train Loss: 0.1616 | Val Loss: 0.1613\n",
      "Epoch 91/200 — Train Loss: 0.1612 | Val Loss: 0.1604\n",
      "Epoch 92/200 — Train Loss: 0.1610 | Val Loss: 0.1597\n",
      "Epoch 93/200 — Train Loss: 0.1607 | Val Loss: 0.1608\n",
      "Epoch 94/200 — Train Loss: 0.1603 | Val Loss: 0.1604\n",
      "Epoch 95/200 — Train Loss: 0.1602 | Val Loss: 0.1583\n",
      "Epoch 96/200 — Train Loss: 0.1597 | Val Loss: 0.1592\n",
      "Epoch 97/200 — Train Loss: 0.1594 | Val Loss: 0.1590\n",
      "Epoch 98/200 — Train Loss: 0.1593 | Val Loss: 0.1602\n",
      "Epoch 99/200 — Train Loss: 0.1591 | Val Loss: 0.1594\n",
      "Epoch 100/200 — Train Loss: 0.1590 | Val Loss: 0.1578\n",
      "Epoch 101/200 — Train Loss: 0.1588 | Val Loss: 0.1589\n",
      "Epoch 102/200 — Train Loss: 0.1583 | Val Loss: 0.1594\n",
      "Epoch 103/200 — Train Loss: 0.1581 | Val Loss: 0.1580\n",
      "Epoch 104/200 — Train Loss: 0.1579 | Val Loss: 0.1575\n",
      "Epoch 105/200 — Train Loss: 0.1576 | Val Loss: 0.1560\n",
      "Epoch 106/200 — Train Loss: 0.1572 | Val Loss: 0.1573\n",
      "Epoch 107/200 — Train Loss: 0.1571 | Val Loss: 0.1551\n",
      "Epoch 108/200 — Train Loss: 0.1575 | Val Loss: 0.1562\n",
      "Epoch 109/200 — Train Loss: 0.1570 | Val Loss: 0.1548\n",
      "Epoch 110/200 — Train Loss: 0.1565 | Val Loss: 0.1568\n",
      "Epoch 111/200 — Train Loss: 0.1562 | Val Loss: 0.1562\n",
      "Epoch 112/200 — Train Loss: 0.1559 | Val Loss: 0.1560\n",
      "Epoch 113/200 — Train Loss: 0.1559 | Val Loss: 0.1566\n",
      "Epoch 114/200 — Train Loss: 0.1556 | Val Loss: 0.1536\n",
      "Epoch 115/200 — Train Loss: 0.1557 | Val Loss: 0.1541\n",
      "Epoch 116/200 — Train Loss: 0.1552 | Val Loss: 0.1554\n",
      "Epoch 117/200 — Train Loss: 0.1545 | Val Loss: 0.1540\n",
      "Epoch 118/200 — Train Loss: 0.1547 | Val Loss: 0.1539\n",
      "Epoch 119/200 — Train Loss: 0.1547 | Val Loss: 0.1538\n",
      "Epoch 120/200 — Train Loss: 0.1544 | Val Loss: 0.1547\n",
      "Epoch 121/200 — Train Loss: 0.1542 | Val Loss: 0.1542\n",
      "Epoch 122/200 — Train Loss: 0.1539 | Val Loss: 0.1528\n",
      "Epoch 123/200 — Train Loss: 0.1537 | Val Loss: 0.1517\n",
      "Epoch 124/200 — Train Loss: 0.1534 | Val Loss: 0.1536\n",
      "Epoch 125/200 — Train Loss: 0.1536 | Val Loss: 0.1515\n",
      "Epoch 126/200 — Train Loss: 0.1527 | Val Loss: 0.1521\n",
      "Epoch 127/200 — Train Loss: 0.1530 | Val Loss: 0.1540\n",
      "Epoch 128/200 — Train Loss: 0.1529 | Val Loss: 0.1531\n",
      "Epoch 129/200 — Train Loss: 0.1525 | Val Loss: 0.1527\n",
      "Epoch 130/200 — Train Loss: 0.1525 | Val Loss: 0.1513\n",
      "Epoch 131/200 — Train Loss: 0.1526 | Val Loss: 0.1519\n",
      "Epoch 132/200 — Train Loss: 0.1521 | Val Loss: 0.1523\n",
      "Epoch 133/200 — Train Loss: 0.1517 | Val Loss: 0.1503\n",
      "Epoch 134/200 — Train Loss: 0.1516 | Val Loss: 0.1496\n",
      "Epoch 135/200 — Train Loss: 0.1513 | Val Loss: 0.1511\n",
      "Epoch 136/200 — Train Loss: 0.1521 | Val Loss: 0.1507\n",
      "Epoch 137/200 — Train Loss: 0.1513 | Val Loss: 0.1506\n",
      "Epoch 138/200 — Train Loss: 0.1510 | Val Loss: 0.1508\n",
      "Epoch 139/200 — Train Loss: 0.1507 | Val Loss: 0.1513\n",
      "Epoch 140/200 — Train Loss: 0.1509 | Val Loss: 0.1499\n",
      "Epoch 141/200 — Train Loss: 0.1504 | Val Loss: 0.1484\n",
      "Epoch 142/200 — Train Loss: 0.1502 | Val Loss: 0.1498\n",
      "Epoch 143/200 — Train Loss: 0.1501 | Val Loss: 0.1493\n",
      "Epoch 144/200 — Train Loss: 0.1503 | Val Loss: 0.1504\n",
      "Epoch 145/200 — Train Loss: 0.1498 | Val Loss: 0.1495\n",
      "Epoch 146/200 — Train Loss: 0.1499 | Val Loss: 0.1487\n",
      "Epoch 147/200 — Train Loss: 0.1493 | Val Loss: 0.1510\n",
      "Epoch 148/200 — Train Loss: 0.1488 | Val Loss: 0.1486\n",
      "Epoch 149/200 — Train Loss: 0.1494 | Val Loss: 0.1485\n",
      "Epoch 150/200 — Train Loss: 0.1491 | Val Loss: 0.1491\n",
      "Epoch 151/200 — Train Loss: 0.1489 | Val Loss: 0.1467\n",
      "Epoch 152/200 — Train Loss: 0.1490 | Val Loss: 0.1497\n",
      "Epoch 153/200 — Train Loss: 0.1487 | Val Loss: 0.1489\n",
      "Epoch 154/200 — Train Loss: 0.1484 | Val Loss: 0.1483\n",
      "Epoch 155/200 — Train Loss: 0.1486 | Val Loss: 0.1486\n",
      "Epoch 156/200 — Train Loss: 0.1477 | Val Loss: 0.1467\n",
      "Epoch 157/200 — Train Loss: 0.1483 | Val Loss: 0.1461\n",
      "Epoch 158/200 — Train Loss: 0.1477 | Val Loss: 0.1480\n",
      "Epoch 159/200 — Train Loss: 0.1477 | Val Loss: 0.1465\n",
      "Epoch 160/200 — Train Loss: 0.1478 | Val Loss: 0.1465\n",
      "Epoch 161/200 — Train Loss: 0.1474 | Val Loss: 0.1459\n",
      "Epoch 162/200 — Train Loss: 0.1471 | Val Loss: 0.1450\n",
      "Epoch 163/200 — Train Loss: 0.1473 | Val Loss: 0.1453\n",
      "Epoch 164/200 — Train Loss: 0.1473 | Val Loss: 0.1491\n",
      "Epoch 165/200 — Train Loss: 0.1475 | Val Loss: 0.1469\n",
      "Epoch 166/200 — Train Loss: 0.1472 | Val Loss: 0.1471\n",
      "Epoch 167/200 — Train Loss: 0.1470 | Val Loss: 0.1476\n",
      "Epoch 168/200 — Train Loss: 0.1468 | Val Loss: 0.1462\n",
      "Epoch 169/200 — Train Loss: 0.1464 | Val Loss: 0.1468\n",
      "Epoch 170/200 — Train Loss: 0.1467 | Val Loss: 0.1473\n",
      "Epoch 171/200 — Train Loss: 0.1463 | Val Loss: 0.1466\n",
      "Epoch 172/200 — Train Loss: 0.1460 | Val Loss: 0.1452\n",
      "Epoch 173/200 — Train Loss: 0.1459 | Val Loss: 0.1449\n",
      "Epoch 174/200 — Train Loss: 0.1457 | Val Loss: 0.1445\n",
      "Epoch 175/200 — Train Loss: 0.1458 | Val Loss: 0.1457\n",
      "Epoch 176/200 — Train Loss: 0.1456 | Val Loss: 0.1438\n",
      "Epoch 177/200 — Train Loss: 0.1459 | Val Loss: 0.1452\n",
      "Epoch 178/200 — Train Loss: 0.1455 | Val Loss: 0.1454\n",
      "Epoch 179/200 — Train Loss: 0.1457 | Val Loss: 0.1465\n",
      "Epoch 180/200 — Train Loss: 0.1453 | Val Loss: 0.1430\n",
      "Epoch 181/200 — Train Loss: 0.1450 | Val Loss: 0.1460\n",
      "Epoch 182/200 — Train Loss: 0.1454 | Val Loss: 0.1448\n",
      "Epoch 183/200 — Train Loss: 0.1449 | Val Loss: 0.1443\n",
      "Epoch 184/200 — Train Loss: 0.1451 | Val Loss: 0.1432\n",
      "Epoch 185/200 — Train Loss: 0.1447 | Val Loss: 0.1423\n",
      "Epoch 186/200 — Train Loss: 0.1442 | Val Loss: 0.1435\n",
      "Epoch 187/200 — Train Loss: 0.1446 | Val Loss: 0.1432\n",
      "Epoch 188/200 — Train Loss: 0.1441 | Val Loss: 0.1434\n",
      "Epoch 189/200 — Train Loss: 0.1442 | Val Loss: 0.1439\n",
      "Epoch 190/200 — Train Loss: 0.1440 | Val Loss: 0.1434\n",
      "Epoch 191/200 — Train Loss: 0.1442 | Val Loss: 0.1444\n",
      "Epoch 192/200 — Train Loss: 0.1440 | Val Loss: 0.1422\n",
      "Epoch 193/200 — Train Loss: 0.1435 | Val Loss: 0.1461\n",
      "Epoch 194/200 — Train Loss: 0.1441 | Val Loss: 0.1434\n",
      "Epoch 195/200 — Train Loss: 0.1434 | Val Loss: 0.1417\n",
      "Epoch 196/200 — Train Loss: 0.1432 | Val Loss: 0.1419\n",
      "Epoch 197/200 — Train Loss: 0.1435 | Val Loss: 0.1428\n",
      "Epoch 198/200 — Train Loss: 0.1429 | Val Loss: 0.1420\n",
      "Epoch 199/200 — Train Loss: 0.1432 | Val Loss: 0.1427\n",
      "Epoch 200/200 — Train Loss: 0.1434 | Val Loss: 0.1435\n"
     ]
    }
   ],
   "source": [
    "# ─── 8) Define & train MLP on synthetic ────────────────────────────────────────\n",
    "class PresenceNet(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(D,128), nn.ReLU(),\n",
    "            nn.Linear(128,64), nn.ReLU(),\n",
    "            nn.Linear(64,C),   nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "D = syn_embeds.shape[1]\n",
    "model_p = PresenceNet(D, C)\n",
    "criterion = nn.BCELoss()\n",
    "opt = optim.Adam(model_p.parameters(), lr=1e-3)\n",
    "epochs = 200\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    model_p.train()\n",
    "    loss_tr = 0\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model_p(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        loss_tr += loss.item()*xb.size(0)\n",
    "    loss_tr /= len(train_loader.dataset)\n",
    "    model_p.eval()\n",
    "    loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            loss_val += criterion(model_p(xb), yb).item()*xb.size(0)\n",
    "    loss_val /= len(val_loader.dataset)\n",
    "    print(f\"Epoch {e}/{epochs} — Train Loss: {loss_tr:.4f} | Val Loss: {loss_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4495ee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synthetic Test Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "           1,9-nonanedithiol       0.99      0.71      0.83       266\n",
      "             1-dodecanethiol       0.63      0.56      0.59       226\n",
      "             1-undecanethiol       0.59      0.30      0.40       224\n",
      "        6-mercapto-1-hexanol       0.85      0.63      0.72       441\n",
      "                     benzene       1.00      0.95      0.97       238\n",
      "                benzenethiol       0.96      0.82      0.88       269\n",
      "                        dmmp       1.00      0.87      0.93       257\n",
      "                        etoh       0.98      0.71      0.82       246\n",
      "                        meoh       0.89      0.85      0.87       414\n",
      "       n,n-dimethylformamide       0.96      0.77      0.86       244\n",
      "                    pyridine       0.94      0.90      0.91       258\n",
      "tris(2-ethylhexyl) phosphate       0.97      0.72      0.83       223\n",
      "\n",
      "                   micro avg       0.90      0.74      0.81      3306\n",
      "                   macro avg       0.90      0.73      0.80      3306\n",
      "                weighted avg       0.90      0.74      0.80      3306\n",
      "                 samples avg       0.93      0.74      0.80      3306\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\levinej\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# ─── 9) Evaluate synthetic test set ────────────────────────────────────────────\n",
    "model_p.eval()\n",
    "yp, yt = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        yp.append(model_p(xb).numpy())\n",
    "        yt.append(yb.numpy())\n",
    "y_pred = (np.vstack(yp)>0.5).astype(int)\n",
    "y_true = np.vstack(yt)\n",
    "print(\"\\nSynthetic Test Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19fe6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 10) Final validation on real mixtures ─────────────────────────────────────\n",
    "mix_df = pd.read_csv('mixtures_dataset.csv')\n",
    "floatify_cols(mix_df)\n",
    "mix_specs = mix_df[wav_cols].values\n",
    "mix_proc  = preprocess(mix_specs)\n",
    "pairs     = list(zip(mix_df['Label 1'], mix_df['Label 2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25f1b58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real Mixtures Validation Report (labels with support > 0):\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "      1-dodecanethiol       1.00      0.79      0.88       243\n",
      " 6-mercapto-1-hexanol       1.00      0.33      0.50       108\n",
      "              benzene       1.00      1.00      1.00       193\n",
      "         benzenethiol       0.96      1.00      0.98        72\n",
      "                 etoh       0.99      0.70      0.82       121\n",
      "                 meoh       1.00      0.79      0.89       243\n",
      "n,n-dimethylformamide       1.00      1.00      1.00        72\n",
      "             pyridine       1.00      1.00      1.00       108\n",
      "\n",
      "            micro avg       1.00      0.82      0.90      1160\n",
      "            macro avg       0.99      0.83      0.88      1160\n",
      "         weighted avg       1.00      0.82      0.88      1160\n",
      "          samples avg       1.00      0.82      0.88      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mix_embeds = siamese(torch.tensor(mix_proc, dtype=torch.float32).unsqueeze(1)).cpu().numpy()\n",
    "\n",
    "# build real multi-hot\n",
    "N_real = len(mix_df)\n",
    "Y_real = np.zeros((N_real, C), dtype=int)\n",
    "for i, (l1, l2) in enumerate(pairs):\n",
    "    Y_real[i, class_to_i[l1]] = 1\n",
    "    Y_real[i, class_to_i[l2]] = 1\n",
    "\n",
    "# predict real\n",
    "model_p.eval()\n",
    "preds = model_p(torch.tensor(mix_embeds, dtype=torch.float32)).detach().numpy()\n",
    "Y_pred_real = (preds>0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1) Compute support for each class\n",
    "supports = Y_real.sum(axis=0)   # length C array of counts\n",
    "\n",
    "# 2) Select only the classes with support > 0\n",
    "valid_idx = [i for i, s in enumerate(supports) if s > 0]\n",
    "valid_labels = [classes[i] for i in valid_idx]\n",
    "\n",
    "# 3) Filter y_true and y_pred to these columns\n",
    "y_true_filt = Y_real[:, valid_idx]\n",
    "y_pred_filt = Y_pred_real[:, valid_idx]\n",
    "\n",
    "# 4) Print report on the filtered set\n",
    "print(\"\\nReal Mixtures Validation Report (labels with support > 0):\")\n",
    "print(classification_report(\n",
    "    y_true_filt,\n",
    "    y_pred_filt,\n",
    "    target_names=valid_labels,\n",
    "    zero_division=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b85c0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define the PresenceNet WITHOUT final Sigmoid\n",
    "class PresenceNetLogits(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(D, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, C)   # raw logits\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2db2ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Instantiate model\n",
    "D = X_train_s.shape[1]  # embedding dimension\n",
    "C = len(classes)\n",
    "model_boost = PresenceNetLogits(D, C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6f5cd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight: tensor([5.7298, 5.6087, 5.5336, 2.9891, 5.7401, 5.7711, 5.7815, 5.7366, 2.9381,\n",
      "        5.6386, 5.7366, 5.6054])\n"
     ]
    }
   ],
   "source": [
    "# 3) Build pos_weight to up-weight specific classes\n",
    "\n",
    "pos = Y_train_s.sum(axis=0)\n",
    "neg = len(Y_train_s) - pos\n",
    "pos_weight = torch.tensor((neg/pos).clip(min=1.0), dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(\"pos_weight:\", pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e366047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 — Train Loss: 0.9252 | Val Loss: 0.7790\n",
      "Epoch 2/200 — Train Loss: 0.7477 | Val Loss: 0.7370\n",
      "Epoch 3/200 — Train Loss: 0.7107 | Val Loss: 0.6995\n",
      "Epoch 4/200 — Train Loss: 0.6785 | Val Loss: 0.6736\n",
      "Epoch 5/200 — Train Loss: 0.6561 | Val Loss: 0.6537\n",
      "Epoch 6/200 — Train Loss: 0.6393 | Val Loss: 0.6372\n",
      "Epoch 7/200 — Train Loss: 0.6262 | Val Loss: 0.6267\n",
      "Epoch 8/200 — Train Loss: 0.6154 | Val Loss: 0.6141\n",
      "Epoch 9/200 — Train Loss: 0.6065 | Val Loss: 0.6074\n",
      "Epoch 10/200 — Train Loss: 0.5980 | Val Loss: 0.5991\n",
      "Epoch 11/200 — Train Loss: 0.5903 | Val Loss: 0.5905\n",
      "Epoch 12/200 — Train Loss: 0.5849 | Val Loss: 0.5832\n",
      "Epoch 13/200 — Train Loss: 0.5789 | Val Loss: 0.5799\n",
      "Epoch 14/200 — Train Loss: 0.5726 | Val Loss: 0.5716\n",
      "Epoch 15/200 — Train Loss: 0.5677 | Val Loss: 0.5653\n",
      "Epoch 16/200 — Train Loss: 0.5625 | Val Loss: 0.5654\n",
      "Epoch 17/200 — Train Loss: 0.5579 | Val Loss: 0.5572\n",
      "Epoch 18/200 — Train Loss: 0.5533 | Val Loss: 0.5518\n",
      "Epoch 19/200 — Train Loss: 0.5487 | Val Loss: 0.5506\n",
      "Epoch 20/200 — Train Loss: 0.5434 | Val Loss: 0.5431\n",
      "Epoch 21/200 — Train Loss: 0.5398 | Val Loss: 0.5384\n",
      "Epoch 22/200 — Train Loss: 0.5349 | Val Loss: 0.5336\n",
      "Epoch 23/200 — Train Loss: 0.5308 | Val Loss: 0.5296\n",
      "Epoch 24/200 — Train Loss: 0.5266 | Val Loss: 0.5269\n",
      "Epoch 25/200 — Train Loss: 0.5225 | Val Loss: 0.5266\n",
      "Epoch 26/200 — Train Loss: 0.5180 | Val Loss: 0.5182\n",
      "Epoch 27/200 — Train Loss: 0.5134 | Val Loss: 0.5169\n",
      "Epoch 28/200 — Train Loss: 0.5103 | Val Loss: 0.5124\n",
      "Epoch 29/200 — Train Loss: 0.5060 | Val Loss: 0.5090\n",
      "Epoch 30/200 — Train Loss: 0.5037 | Val Loss: 0.5059\n",
      "Epoch 31/200 — Train Loss: 0.5009 | Val Loss: 0.4987\n",
      "Epoch 32/200 — Train Loss: 0.4960 | Val Loss: 0.4971\n",
      "Epoch 33/200 — Train Loss: 0.4927 | Val Loss: 0.4931\n",
      "Epoch 34/200 — Train Loss: 0.4896 | Val Loss: 0.4952\n",
      "Epoch 35/200 — Train Loss: 0.4866 | Val Loss: 0.4883\n",
      "Epoch 36/200 — Train Loss: 0.4831 | Val Loss: 0.4874\n",
      "Epoch 37/200 — Train Loss: 0.4805 | Val Loss: 0.4908\n",
      "Epoch 38/200 — Train Loss: 0.4778 | Val Loss: 0.4797\n",
      "Epoch 39/200 — Train Loss: 0.4748 | Val Loss: 0.4789\n",
      "Epoch 40/200 — Train Loss: 0.4720 | Val Loss: 0.4756\n",
      "Epoch 41/200 — Train Loss: 0.4697 | Val Loss: 0.4699\n",
      "Epoch 42/200 — Train Loss: 0.4673 | Val Loss: 0.4748\n",
      "Epoch 43/200 — Train Loss: 0.4651 | Val Loss: 0.4656\n",
      "Epoch 44/200 — Train Loss: 0.4624 | Val Loss: 0.4682\n",
      "Epoch 45/200 — Train Loss: 0.4610 | Val Loss: 0.4633\n",
      "Epoch 46/200 — Train Loss: 0.4579 | Val Loss: 0.4626\n",
      "Epoch 47/200 — Train Loss: 0.4567 | Val Loss: 0.4567\n",
      "Epoch 48/200 — Train Loss: 0.4541 | Val Loss: 0.4565\n",
      "Epoch 49/200 — Train Loss: 0.4522 | Val Loss: 0.4563\n",
      "Epoch 50/200 — Train Loss: 0.4493 | Val Loss: 0.4561\n",
      "Epoch 51/200 — Train Loss: 0.4483 | Val Loss: 0.4525\n",
      "Epoch 52/200 — Train Loss: 0.4458 | Val Loss: 0.4466\n",
      "Epoch 53/200 — Train Loss: 0.4445 | Val Loss: 0.4491\n",
      "Epoch 54/200 — Train Loss: 0.4427 | Val Loss: 0.4456\n",
      "Epoch 55/200 — Train Loss: 0.4416 | Val Loss: 0.4446\n",
      "Epoch 56/200 — Train Loss: 0.4401 | Val Loss: 0.4417\n",
      "Epoch 57/200 — Train Loss: 0.4382 | Val Loss: 0.4394\n",
      "Epoch 58/200 — Train Loss: 0.4360 | Val Loss: 0.4361\n",
      "Epoch 59/200 — Train Loss: 0.4343 | Val Loss: 0.4383\n",
      "Epoch 60/200 — Train Loss: 0.4330 | Val Loss: 0.4330\n",
      "Epoch 61/200 — Train Loss: 0.4314 | Val Loss: 0.4359\n",
      "Epoch 62/200 — Train Loss: 0.4315 | Val Loss: 0.4339\n",
      "Epoch 63/200 — Train Loss: 0.4290 | Val Loss: 0.4296\n",
      "Epoch 64/200 — Train Loss: 0.4274 | Val Loss: 0.4278\n",
      "Epoch 65/200 — Train Loss: 0.4264 | Val Loss: 0.4306\n",
      "Epoch 66/200 — Train Loss: 0.4248 | Val Loss: 0.4303\n",
      "Epoch 67/200 — Train Loss: 0.4243 | Val Loss: 0.4244\n",
      "Epoch 68/200 — Train Loss: 0.4222 | Val Loss: 0.4208\n",
      "Epoch 69/200 — Train Loss: 0.4221 | Val Loss: 0.4251\n",
      "Epoch 70/200 — Train Loss: 0.4210 | Val Loss: 0.4218\n",
      "Epoch 71/200 — Train Loss: 0.4190 | Val Loss: 0.4220\n",
      "Epoch 72/200 — Train Loss: 0.4173 | Val Loss: 0.4206\n",
      "Epoch 73/200 — Train Loss: 0.4174 | Val Loss: 0.4222\n",
      "Epoch 74/200 — Train Loss: 0.4157 | Val Loss: 0.4157\n",
      "Epoch 75/200 — Train Loss: 0.4144 | Val Loss: 0.4154\n",
      "Epoch 76/200 — Train Loss: 0.4139 | Val Loss: 0.4181\n",
      "Epoch 77/200 — Train Loss: 0.4126 | Val Loss: 0.4164\n",
      "Epoch 78/200 — Train Loss: 0.4122 | Val Loss: 0.4128\n",
      "Epoch 79/200 — Train Loss: 0.4100 | Val Loss: 0.4134\n",
      "Epoch 80/200 — Train Loss: 0.4102 | Val Loss: 0.4144\n",
      "Epoch 81/200 — Train Loss: 0.4092 | Val Loss: 0.4148\n",
      "Epoch 82/200 — Train Loss: 0.4077 | Val Loss: 0.4116\n",
      "Epoch 83/200 — Train Loss: 0.4068 | Val Loss: 0.4126\n",
      "Epoch 84/200 — Train Loss: 0.4073 | Val Loss: 0.4062\n",
      "Epoch 85/200 — Train Loss: 0.4049 | Val Loss: 0.4072\n",
      "Epoch 86/200 — Train Loss: 0.4041 | Val Loss: 0.4092\n",
      "Epoch 87/200 — Train Loss: 0.4041 | Val Loss: 0.4061\n",
      "Epoch 88/200 — Train Loss: 0.4027 | Val Loss: 0.4043\n",
      "Epoch 89/200 — Train Loss: 0.4016 | Val Loss: 0.4066\n",
      "Epoch 90/200 — Train Loss: 0.4011 | Val Loss: 0.4074\n",
      "Epoch 91/200 — Train Loss: 0.4001 | Val Loss: 0.4011\n",
      "Epoch 92/200 — Train Loss: 0.3994 | Val Loss: 0.4038\n",
      "Epoch 93/200 — Train Loss: 0.3982 | Val Loss: 0.3993\n",
      "Epoch 94/200 — Train Loss: 0.3974 | Val Loss: 0.3988\n",
      "Epoch 95/200 — Train Loss: 0.3970 | Val Loss: 0.3995\n",
      "Epoch 96/200 — Train Loss: 0.3968 | Val Loss: 0.3981\n",
      "Epoch 97/200 — Train Loss: 0.3964 | Val Loss: 0.3968\n",
      "Epoch 98/200 — Train Loss: 0.3952 | Val Loss: 0.3974\n",
      "Epoch 99/200 — Train Loss: 0.3952 | Val Loss: 0.3968\n",
      "Epoch 100/200 — Train Loss: 0.3942 | Val Loss: 0.3944\n",
      "Epoch 101/200 — Train Loss: 0.3923 | Val Loss: 0.4037\n",
      "Epoch 102/200 — Train Loss: 0.3929 | Val Loss: 0.3930\n",
      "Epoch 103/200 — Train Loss: 0.3921 | Val Loss: 0.3965\n",
      "Epoch 104/200 — Train Loss: 0.3911 | Val Loss: 0.3949\n",
      "Epoch 105/200 — Train Loss: 0.3906 | Val Loss: 0.3898\n",
      "Epoch 106/200 — Train Loss: 0.3904 | Val Loss: 0.3946\n",
      "Epoch 107/200 — Train Loss: 0.3886 | Val Loss: 0.3953\n",
      "Epoch 108/200 — Train Loss: 0.3893 | Val Loss: 0.3943\n",
      "Epoch 109/200 — Train Loss: 0.3895 | Val Loss: 0.3905\n",
      "Epoch 110/200 — Train Loss: 0.3886 | Val Loss: 0.3884\n",
      "Epoch 111/200 — Train Loss: 0.3875 | Val Loss: 0.3953\n",
      "Epoch 112/200 — Train Loss: 0.3868 | Val Loss: 0.3954\n",
      "Epoch 113/200 — Train Loss: 0.3864 | Val Loss: 0.3873\n",
      "Epoch 114/200 — Train Loss: 0.3856 | Val Loss: 0.3937\n",
      "Epoch 115/200 — Train Loss: 0.3856 | Val Loss: 0.3863\n",
      "Epoch 116/200 — Train Loss: 0.3836 | Val Loss: 0.3856\n",
      "Epoch 117/200 — Train Loss: 0.3840 | Val Loss: 0.3864\n",
      "Epoch 118/200 — Train Loss: 0.3837 | Val Loss: 0.3893\n",
      "Epoch 119/200 — Train Loss: 0.3825 | Val Loss: 0.3876\n",
      "Epoch 120/200 — Train Loss: 0.3816 | Val Loss: 0.3866\n",
      "Epoch 121/200 — Train Loss: 0.3822 | Val Loss: 0.3886\n",
      "Epoch 122/200 — Train Loss: 0.3817 | Val Loss: 0.3898\n",
      "Epoch 123/200 — Train Loss: 0.3817 | Val Loss: 0.3903\n",
      "Epoch 124/200 — Train Loss: 0.3808 | Val Loss: 0.3859\n",
      "Epoch 125/200 — Train Loss: 0.3797 | Val Loss: 0.3821\n",
      "Epoch 126/200 — Train Loss: 0.3803 | Val Loss: 0.3891\n",
      "Epoch 127/200 — Train Loss: 0.3782 | Val Loss: 0.3831\n",
      "Epoch 128/200 — Train Loss: 0.3797 | Val Loss: 0.3876\n",
      "Epoch 129/200 — Train Loss: 0.3773 | Val Loss: 0.3840\n",
      "Epoch 130/200 — Train Loss: 0.3787 | Val Loss: 0.3846\n",
      "Epoch 131/200 — Train Loss: 0.3783 | Val Loss: 0.3823\n",
      "Epoch 132/200 — Train Loss: 0.3763 | Val Loss: 0.3837\n",
      "Epoch 133/200 — Train Loss: 0.3765 | Val Loss: 0.3821\n",
      "Epoch 134/200 — Train Loss: 0.3757 | Val Loss: 0.3817\n",
      "Epoch 135/200 — Train Loss: 0.3754 | Val Loss: 0.3803\n",
      "Epoch 136/200 — Train Loss: 0.3762 | Val Loss: 0.3790\n",
      "Epoch 137/200 — Train Loss: 0.3744 | Val Loss: 0.3768\n",
      "Epoch 138/200 — Train Loss: 0.3731 | Val Loss: 0.3791\n",
      "Epoch 139/200 — Train Loss: 0.3747 | Val Loss: 0.3754\n",
      "Epoch 140/200 — Train Loss: 0.3734 | Val Loss: 0.3790\n",
      "Epoch 141/200 — Train Loss: 0.3723 | Val Loss: 0.3819\n",
      "Epoch 142/200 — Train Loss: 0.3729 | Val Loss: 0.3744\n",
      "Epoch 143/200 — Train Loss: 0.3724 | Val Loss: 0.3775\n",
      "Epoch 144/200 — Train Loss: 0.3728 | Val Loss: 0.3739\n",
      "Epoch 145/200 — Train Loss: 0.3710 | Val Loss: 0.3787\n",
      "Epoch 146/200 — Train Loss: 0.3715 | Val Loss: 0.3744\n",
      "Epoch 147/200 — Train Loss: 0.3711 | Val Loss: 0.3743\n",
      "Epoch 148/200 — Train Loss: 0.3705 | Val Loss: 0.3765\n",
      "Epoch 149/200 — Train Loss: 0.3709 | Val Loss: 0.3740\n",
      "Epoch 150/200 — Train Loss: 0.3705 | Val Loss: 0.3704\n",
      "Epoch 151/200 — Train Loss: 0.3694 | Val Loss: 0.3747\n",
      "Epoch 152/200 — Train Loss: 0.3689 | Val Loss: 0.3743\n",
      "Epoch 153/200 — Train Loss: 0.3686 | Val Loss: 0.3712\n",
      "Epoch 154/200 — Train Loss: 0.3679 | Val Loss: 0.3716\n",
      "Epoch 155/200 — Train Loss: 0.3668 | Val Loss: 0.3775\n",
      "Epoch 156/200 — Train Loss: 0.3667 | Val Loss: 0.3740\n",
      "Epoch 157/200 — Train Loss: 0.3680 | Val Loss: 0.3706\n",
      "Epoch 158/200 — Train Loss: 0.3667 | Val Loss: 0.3760\n",
      "Epoch 159/200 — Train Loss: 0.3654 | Val Loss: 0.3760\n",
      "Epoch 160/200 — Train Loss: 0.3658 | Val Loss: 0.3696\n",
      "Epoch 161/200 — Train Loss: 0.3651 | Val Loss: 0.3673\n",
      "Epoch 162/200 — Train Loss: 0.3648 | Val Loss: 0.3678\n",
      "Epoch 163/200 — Train Loss: 0.3651 | Val Loss: 0.3659\n",
      "Epoch 164/200 — Train Loss: 0.3641 | Val Loss: 0.3723\n",
      "Epoch 165/200 — Train Loss: 0.3642 | Val Loss: 0.3688\n",
      "Epoch 166/200 — Train Loss: 0.3643 | Val Loss: 0.3675\n",
      "Epoch 167/200 — Train Loss: 0.3633 | Val Loss: 0.3780\n",
      "Epoch 168/200 — Train Loss: 0.3640 | Val Loss: 0.3676\n",
      "Epoch 169/200 — Train Loss: 0.3634 | Val Loss: 0.3690\n",
      "Epoch 170/200 — Train Loss: 0.3628 | Val Loss: 0.3643\n",
      "Epoch 171/200 — Train Loss: 0.3624 | Val Loss: 0.3647\n",
      "Epoch 172/200 — Train Loss: 0.3623 | Val Loss: 0.3686\n",
      "Epoch 173/200 — Train Loss: 0.3626 | Val Loss: 0.3693\n",
      "Epoch 174/200 — Train Loss: 0.3626 | Val Loss: 0.3629\n",
      "Epoch 175/200 — Train Loss: 0.3622 | Val Loss: 0.3688\n",
      "Epoch 176/200 — Train Loss: 0.3609 | Val Loss: 0.3662\n",
      "Epoch 177/200 — Train Loss: 0.3611 | Val Loss: 0.3728\n",
      "Epoch 178/200 — Train Loss: 0.3615 | Val Loss: 0.3650\n",
      "Epoch 179/200 — Train Loss: 0.3603 | Val Loss: 0.3654\n",
      "Epoch 180/200 — Train Loss: 0.3591 | Val Loss: 0.3686\n",
      "Epoch 181/200 — Train Loss: 0.3613 | Val Loss: 0.3636\n",
      "Epoch 182/200 — Train Loss: 0.3598 | Val Loss: 0.3658\n",
      "Epoch 183/200 — Train Loss: 0.3599 | Val Loss: 0.3675\n",
      "Epoch 184/200 — Train Loss: 0.3592 | Val Loss: 0.3636\n",
      "Epoch 185/200 — Train Loss: 0.3577 | Val Loss: 0.3654\n",
      "Epoch 186/200 — Train Loss: 0.3598 | Val Loss: 0.3649\n",
      "Epoch 187/200 — Train Loss: 0.3578 | Val Loss: 0.3680\n",
      "Epoch 188/200 — Train Loss: 0.3581 | Val Loss: 0.3643\n",
      "Epoch 189/200 — Train Loss: 0.3592 | Val Loss: 0.3660\n",
      "Epoch 190/200 — Train Loss: 0.3586 | Val Loss: 0.3619\n",
      "Epoch 191/200 — Train Loss: 0.3581 | Val Loss: 0.3644\n",
      "Epoch 192/200 — Train Loss: 0.3562 | Val Loss: 0.3645\n",
      "Epoch 193/200 — Train Loss: 0.3572 | Val Loss: 0.3650\n",
      "Epoch 194/200 — Train Loss: 0.3572 | Val Loss: 0.3637\n",
      "Epoch 195/200 — Train Loss: 0.3565 | Val Loss: 0.3633\n",
      "Epoch 196/200 — Train Loss: 0.3571 | Val Loss: 0.3588\n",
      "Epoch 197/200 — Train Loss: 0.3556 | Val Loss: 0.3607\n",
      "Epoch 198/200 — Train Loss: 0.3543 | Val Loss: 0.3595\n",
      "Epoch 199/200 — Train Loss: 0.3552 | Val Loss: 0.3572\n",
      "Epoch 200/200 — Train Loss: 0.3541 | Val Loss: 0.3595\n"
     ]
    }
   ],
   "source": [
    "# 4) Use BCEWithLogitsLoss with pos_weight\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model_boost.parameters(), lr=1e-3)\n",
    "\n",
    "# 5) Training loop skeleton\n",
    "num_epochs = 200\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model_boost.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        logits = model_boost(xb)\n",
    "        loss = criterion(logits, yb)  # yb must be FloatTensor\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation...\n",
    "    model_boost.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            logits = model_boost(xb)\n",
    "            val_loss += criterion(logits, yb).item() * xb.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs} — \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af4656b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real Mixtures Validation Report (labels with support > 0):\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "      1-dodecanethiol       1.00      0.79      0.89       243\n",
      " 6-mercapto-1-hexanol       1.00      0.33      0.50       108\n",
      "              benzene       1.00      1.00      1.00       193\n",
      "         benzenethiol       0.94      1.00      0.97        72\n",
      "                 etoh       0.98      0.70      0.82       121\n",
      "                 meoh       1.00      0.80      0.89       243\n",
      "n,n-dimethylformamide       0.67      1.00      0.80        72\n",
      "             pyridine       1.00      1.00      1.00       108\n",
      "\n",
      "            micro avg       0.96      0.82      0.88      1160\n",
      "            macro avg       0.95      0.83      0.86      1160\n",
      "         weighted avg       0.97      0.82      0.87      1160\n",
      "          samples avg       0.97      0.82      0.87      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mix_embeds = siamese(torch.tensor(mix_proc, dtype=torch.float32).unsqueeze(1)).cpu().numpy()\n",
    "\n",
    "# build real multi-hot\n",
    "N_real = len(mix_df)\n",
    "Y_real = np.zeros((N_real, C), dtype=int)\n",
    "for i, (l1, l2) in enumerate(pairs):\n",
    "    Y_real[i, class_to_i[l1]] = 1\n",
    "    Y_real[i, class_to_i[l2]] = 1\n",
    "\n",
    "# predict real\n",
    "model_boost.eval()\n",
    "preds = model_boost(torch.tensor(mix_embeds, dtype=torch.float32)).detach().numpy()\n",
    "Y_pred_real = (preds>0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1) Compute support for each class\n",
    "supports = Y_real.sum(axis=0)   # length C array of counts\n",
    "\n",
    "# 2) Select only the classes with support > 0\n",
    "valid_idx = [i for i, s in enumerate(supports) if s > 0]\n",
    "valid_labels = [classes[i] for i in valid_idx]\n",
    "\n",
    "# 3) Filter y_true and y_pred to these columns\n",
    "y_true_filt = Y_real[:, valid_idx]\n",
    "y_pred_filt = Y_pred_real[:, valid_idx]\n",
    "\n",
    "# 4) Print report on the filtered set\n",
    "print(\"\\nReal Mixtures Validation Report (labels with support > 0):\")\n",
    "print(classification_report(\n",
    "    y_true_filt,\n",
    "    y_pred_filt,\n",
    "    target_names=valid_labels,\n",
    "    zero_division=0\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

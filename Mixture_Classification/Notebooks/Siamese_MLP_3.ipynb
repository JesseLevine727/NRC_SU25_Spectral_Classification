{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df5bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f4ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Utilities ────────────────────────────────────────────────────────────────\n",
    "lam, p, niter = 1e4, 0.01, 10\n",
    "def baseline_als(y):\n",
    "    L = len(y)\n",
    "    D = np.diff(np.eye(L), 2)\n",
    "    D = lam * D.dot(D.T)\n",
    "    w = np.ones(L)\n",
    "    for _ in range(niter):\n",
    "        b = np.linalg.solve(np.diag(w) + D, w * y)\n",
    "        w = p * (y > b) + (1 - p) * (y < b)\n",
    "    return b\n",
    "\n",
    "def preprocess(arr):\n",
    "    out = np.zeros_like(arr)\n",
    "    for i, s in enumerate(arr):\n",
    "        b = baseline_als(s)\n",
    "        c = s - b\n",
    "        norm = np.linalg.norm(c)\n",
    "        out[i] = c / norm if norm > 0 else c\n",
    "    return out\n",
    "\n",
    "def floatify_cols(df):\n",
    "    new = []\n",
    "    for c in df.columns:\n",
    "        if c in ('Label', 'Label 1', 'Label 2'):\n",
    "            new.append(c)\n",
    "        else:\n",
    "            new.append(float(c))\n",
    "    df.columns = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8a890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) Load reference_v2 and preprocess ──────────────────────────────────────\n",
    "ref_df = pd.read_csv('reference_v2.csv')\n",
    "\n",
    "floatify_cols(ref_df)\n",
    "wav_cols = [c for c in ref_df.columns if c != 'Label']\n",
    "ref_specs  = ref_df[wav_cols].values       # (n_ref_samples, n_waves)\n",
    "ref_labels = ref_df['Label'].values        # (n_ref_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce2d604b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic raw spectra: (12540, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Unique chemical classes\n",
    "classes    = sorted(np.unique(ref_labels))\n",
    "C = len(classes)\n",
    "class_to_i = {c:i for i,c in enumerate(classes)}\n",
    "\n",
    "# ─── 2) Generate synthetic mixtures ────────────────────────────────────────────\n",
    "ratios = np.arange(0.05, 1.0, 0.05)\n",
    "noise_level = 0.01\n",
    "n_per_ratio = 10  # number of random spectra per pair/ratio\n",
    "\n",
    "synth_specs = []\n",
    "synth_labels = []\n",
    "for (i, ci), (j, cj) in combinations(enumerate(classes), 2):\n",
    "    # indices of pure spectra for each class\n",
    "    idx_i = np.where(ref_labels == ci)[0]\n",
    "    idx_j = np.where(ref_labels == cj)[0]\n",
    "    for r in ratios:\n",
    "        for _ in range(n_per_ratio):\n",
    "            spec_i = ref_specs[np.random.choice(idx_i)]\n",
    "            spec_j = ref_specs[np.random.choice(idx_j)]\n",
    "            mix = r * spec_i + (1-r) * spec_j\n",
    "            mix += np.random.normal(scale=noise_level, size=mix.shape)\n",
    "            synth_specs.append(mix)\n",
    "            synth_labels.append((ci, cj))\n",
    "synth_specs = np.array(synth_specs)        # (n_synth, n_waves)\n",
    "print(\"Synthetic raw spectra:\", synth_specs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f625eb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 153 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 253 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 309 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 338 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 369 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 400 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done 433 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 466 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done 501 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done 536 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 573 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done 610 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done 649 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 688 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done 770 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done 813 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done 856 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done 901 tasks      | elapsed:   27.3s\n",
      "[Parallel(n_jobs=-1)]: Done 946 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 993 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1040 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1089 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1138 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1189 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1240 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1293 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1346 tasks      | elapsed:   37.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1401 tasks      | elapsed:   38.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1456 tasks      | elapsed:   40.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1513 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1570 tasks      | elapsed:   42.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1629 tasks      | elapsed:   44.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1688 tasks      | elapsed:   45.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1749 tasks      | elapsed:   46.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1810 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1873 tasks      | elapsed:   49.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1936 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2001 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2066 tasks      | elapsed:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2133 tasks      | elapsed:   55.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2200 tasks      | elapsed:   57.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2269 tasks      | elapsed:   58.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2338 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2409 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2480 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2553 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2626 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2701 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2776 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2853 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2930 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3009 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3088 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3169 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3250 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3333 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3416 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3501 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3586 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3673 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3760 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3849 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3938 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4029 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4120 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4213 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4306 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4401 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4496 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4593 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4690 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4789 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4888 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4989 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5090 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5193 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5296 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5401 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5506 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5613 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5720 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5829 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5938 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6049 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6160 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6273 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6386 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6501 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6616 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6733 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6850 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6969 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7088 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7209 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7330 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7453 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7576 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7701 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7826 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7953 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 8080 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8209 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8338 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8469 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8600 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8733 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8866 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9001 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9136 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9273 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9410 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9549 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9688 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9829 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9970 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10113 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10256 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10401 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10546 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10693 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10840 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10989 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11138 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11289 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11440 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11593 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11746 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 11901 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12056 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12213 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 12370 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel preprocess done: (12540, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 12540 out of 12540 | elapsed:  4.9min finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def preprocess_single(spectrum):\n",
    "    \"\"\"\n",
    "    Baseline‐correct + L2‐normalize + abs, for one 1D array.\n",
    "    \"\"\"\n",
    "    b = baseline_als(spectrum)\n",
    "    c = spectrum - b\n",
    "    norm = np.linalg.norm(c)\n",
    "    out = c / norm if norm > 0 else c\n",
    "    return np.abs(out)\n",
    "\n",
    "# 2) Parallel map over all spectra\n",
    "#    n_jobs=-1 uses all CPUs; you can set e.g. n_jobs=4 to use 4 cores.\n",
    "synth_proc = np.vstack(\n",
    "    Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(preprocess_single)(spec) \n",
    "        for spec in synth_specs\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Parallel preprocess done:\", synth_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d70e4a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic embeddings: (12540, 64)\n"
     ]
    }
   ],
   "source": [
    "# ─── 4) Embed synthetic mixtures via Siamese ──────────────────────────────────\n",
    "# load siamese\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, input_len, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1,16,7,padding=3), nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(16,32,5,padding=2), nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_len//4)*32, embed_dim), nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        return F.normalize(z, dim=1)\n",
    "\n",
    "siamese = SiameseNet(input_len=ref_specs.shape[1], embed_dim=64)\n",
    "siamese.load_state_dict(torch.load('siamese_mixture.pth', map_location='cpu'))\n",
    "siamese.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    tensor = torch.tensor(synth_proc, dtype=torch.float32).unsqueeze(1)\n",
    "    syn_embeds = siamese(tensor).cpu().numpy()  # (n_synth, 64)\n",
    "print(\"Synthetic embeddings:\", syn_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b97cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 5) Build X_synth, Y_synth ─────────────────────────────────────────────────\n",
    "N = len(syn_embeds)\n",
    "X_synth = syn_embeds\n",
    "Y_synth = np.zeros((N, C), dtype=int)\n",
    "for k, (ci, cj) in enumerate(synth_labels):\n",
    "    Y_synth[k, class_to_i[ci]] = 1\n",
    "    Y_synth[k, class_to_i[cj]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76c410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic train/val/test: 10032 1254 1254\n"
     ]
    }
   ],
   "source": [
    "# ─── 6) Split synthetic into train/val/test (80/10/10) ─────────────────────────\n",
    "X_tmp, X_test_s, Y_tmp, Y_test_s = train_test_split(X_synth, Y_synth, test_size=0.10, random_state=0)\n",
    "X_train_s, X_val_s, Y_train_s, Y_val_s = train_test_split(X_tmp, Y_tmp, test_size=0.1111, random_state=0)\n",
    "print(\"Synthetic train/val/test:\", len(X_train_s), len(X_val_s), len(X_test_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "589aa2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 7) DataLoaders for synthetic ─────────────────────────────────────────────\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(X_train_s, dtype=torch.float32),\n",
    "        torch.tensor(Y_train_s, dtype=torch.float32)   # ← make this float\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(X_val_s, dtype=torch.float32),\n",
    "        torch.tensor(Y_val_s, dtype=torch.float32)     # ← and this\n",
    "    ),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(X_test_s, dtype=torch.float32),\n",
    "        torch.tensor(Y_test_s, dtype=torch.float32)    # ← and this\n",
    "    ),\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e41175c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 — Train Loss: 0.4826 | Val Loss: 0.3819\n",
      "Epoch 2/200 — Train Loss: 0.3118 | Val Loss: 0.2525\n",
      "Epoch 3/200 — Train Loss: 0.2278 | Val Loss: 0.2089\n",
      "Epoch 4/200 — Train Loss: 0.2023 | Val Loss: 0.1948\n",
      "Epoch 5/200 — Train Loss: 0.1899 | Val Loss: 0.1822\n",
      "Epoch 6/200 — Train Loss: 0.1781 | Val Loss: 0.1716\n",
      "Epoch 7/200 — Train Loss: 0.1673 | Val Loss: 0.1605\n",
      "Epoch 8/200 — Train Loss: 0.1575 | Val Loss: 0.1528\n",
      "Epoch 9/200 — Train Loss: 0.1498 | Val Loss: 0.1463\n",
      "Epoch 10/200 — Train Loss: 0.1438 | Val Loss: 0.1409\n",
      "Epoch 11/200 — Train Loss: 0.1387 | Val Loss: 0.1364\n",
      "Epoch 12/200 — Train Loss: 0.1344 | Val Loss: 0.1326\n",
      "Epoch 13/200 — Train Loss: 0.1308 | Val Loss: 0.1299\n",
      "Epoch 14/200 — Train Loss: 0.1273 | Val Loss: 0.1261\n",
      "Epoch 15/200 — Train Loss: 0.1245 | Val Loss: 0.1236\n",
      "Epoch 16/200 — Train Loss: 0.1215 | Val Loss: 0.1200\n",
      "Epoch 17/200 — Train Loss: 0.1190 | Val Loss: 0.1198\n",
      "Epoch 18/200 — Train Loss: 0.1167 | Val Loss: 0.1171\n",
      "Epoch 19/200 — Train Loss: 0.1148 | Val Loss: 0.1144\n",
      "Epoch 20/200 — Train Loss: 0.1124 | Val Loss: 0.1115\n",
      "Epoch 21/200 — Train Loss: 0.1108 | Val Loss: 0.1102\n",
      "Epoch 22/200 — Train Loss: 0.1092 | Val Loss: 0.1104\n",
      "Epoch 23/200 — Train Loss: 0.1074 | Val Loss: 0.1081\n",
      "Epoch 24/200 — Train Loss: 0.1065 | Val Loss: 0.1062\n",
      "Epoch 25/200 — Train Loss: 0.1049 | Val Loss: 0.1061\n",
      "Epoch 26/200 — Train Loss: 0.1036 | Val Loss: 0.1043\n",
      "Epoch 27/200 — Train Loss: 0.1023 | Val Loss: 0.1029\n",
      "Epoch 28/200 — Train Loss: 0.1012 | Val Loss: 0.1028\n",
      "Epoch 29/200 — Train Loss: 0.1004 | Val Loss: 0.1013\n",
      "Epoch 30/200 — Train Loss: 0.0992 | Val Loss: 0.1000\n",
      "Epoch 31/200 — Train Loss: 0.0983 | Val Loss: 0.0984\n",
      "Epoch 32/200 — Train Loss: 0.0979 | Val Loss: 0.0989\n",
      "Epoch 33/200 — Train Loss: 0.0968 | Val Loss: 0.0973\n",
      "Epoch 34/200 — Train Loss: 0.0961 | Val Loss: 0.0968\n",
      "Epoch 35/200 — Train Loss: 0.0954 | Val Loss: 0.0953\n",
      "Epoch 36/200 — Train Loss: 0.0945 | Val Loss: 0.0960\n",
      "Epoch 37/200 — Train Loss: 0.0938 | Val Loss: 0.0956\n",
      "Epoch 38/200 — Train Loss: 0.0933 | Val Loss: 0.0944\n",
      "Epoch 39/200 — Train Loss: 0.0924 | Val Loss: 0.0942\n",
      "Epoch 40/200 — Train Loss: 0.0920 | Val Loss: 0.0949\n",
      "Epoch 41/200 — Train Loss: 0.0916 | Val Loss: 0.0927\n",
      "Epoch 42/200 — Train Loss: 0.0908 | Val Loss: 0.0919\n",
      "Epoch 43/200 — Train Loss: 0.0903 | Val Loss: 0.0911\n",
      "Epoch 44/200 — Train Loss: 0.0896 | Val Loss: 0.0925\n",
      "Epoch 45/200 — Train Loss: 0.0894 | Val Loss: 0.0912\n",
      "Epoch 46/200 — Train Loss: 0.0890 | Val Loss: 0.0912\n",
      "Epoch 47/200 — Train Loss: 0.0883 | Val Loss: 0.0896\n",
      "Epoch 48/200 — Train Loss: 0.0875 | Val Loss: 0.0892\n",
      "Epoch 49/200 — Train Loss: 0.0876 | Val Loss: 0.0899\n",
      "Epoch 50/200 — Train Loss: 0.0871 | Val Loss: 0.0890\n",
      "Epoch 51/200 — Train Loss: 0.0866 | Val Loss: 0.0891\n",
      "Epoch 52/200 — Train Loss: 0.0861 | Val Loss: 0.0871\n",
      "Epoch 53/200 — Train Loss: 0.0860 | Val Loss: 0.0911\n",
      "Epoch 54/200 — Train Loss: 0.0857 | Val Loss: 0.0863\n",
      "Epoch 55/200 — Train Loss: 0.0849 | Val Loss: 0.0873\n",
      "Epoch 56/200 — Train Loss: 0.0847 | Val Loss: 0.0867\n",
      "Epoch 57/200 — Train Loss: 0.0844 | Val Loss: 0.0864\n",
      "Epoch 58/200 — Train Loss: 0.0836 | Val Loss: 0.0863\n",
      "Epoch 59/200 — Train Loss: 0.0834 | Val Loss: 0.0877\n",
      "Epoch 60/200 — Train Loss: 0.0836 | Val Loss: 0.0849\n",
      "Epoch 61/200 — Train Loss: 0.0828 | Val Loss: 0.0850\n",
      "Epoch 62/200 — Train Loss: 0.0826 | Val Loss: 0.0847\n",
      "Epoch 63/200 — Train Loss: 0.0819 | Val Loss: 0.0836\n",
      "Epoch 64/200 — Train Loss: 0.0821 | Val Loss: 0.0846\n",
      "Epoch 65/200 — Train Loss: 0.0819 | Val Loss: 0.0840\n",
      "Epoch 66/200 — Train Loss: 0.0811 | Val Loss: 0.0840\n",
      "Epoch 67/200 — Train Loss: 0.0809 | Val Loss: 0.0830\n",
      "Epoch 68/200 — Train Loss: 0.0809 | Val Loss: 0.0828\n",
      "Epoch 69/200 — Train Loss: 0.0805 | Val Loss: 0.0825\n",
      "Epoch 70/200 — Train Loss: 0.0801 | Val Loss: 0.0837\n",
      "Epoch 71/200 — Train Loss: 0.0803 | Val Loss: 0.0815\n",
      "Epoch 72/200 — Train Loss: 0.0797 | Val Loss: 0.0835\n",
      "Epoch 73/200 — Train Loss: 0.0795 | Val Loss: 0.0827\n",
      "Epoch 74/200 — Train Loss: 0.0791 | Val Loss: 0.0817\n",
      "Epoch 75/200 — Train Loss: 0.0789 | Val Loss: 0.0820\n",
      "Epoch 76/200 — Train Loss: 0.0788 | Val Loss: 0.0811\n",
      "Epoch 77/200 — Train Loss: 0.0784 | Val Loss: 0.0804\n",
      "Epoch 78/200 — Train Loss: 0.0787 | Val Loss: 0.0797\n",
      "Epoch 79/200 — Train Loss: 0.0783 | Val Loss: 0.0800\n",
      "Epoch 80/200 — Train Loss: 0.0779 | Val Loss: 0.0810\n",
      "Epoch 81/200 — Train Loss: 0.0781 | Val Loss: 0.0794\n",
      "Epoch 82/200 — Train Loss: 0.0773 | Val Loss: 0.0801\n",
      "Epoch 83/200 — Train Loss: 0.0773 | Val Loss: 0.0785\n",
      "Epoch 84/200 — Train Loss: 0.0772 | Val Loss: 0.0800\n",
      "Epoch 85/200 — Train Loss: 0.0769 | Val Loss: 0.0796\n",
      "Epoch 86/200 — Train Loss: 0.0766 | Val Loss: 0.0783\n",
      "Epoch 87/200 — Train Loss: 0.0767 | Val Loss: 0.0774\n",
      "Epoch 88/200 — Train Loss: 0.0763 | Val Loss: 0.0778\n",
      "Epoch 89/200 — Train Loss: 0.0766 | Val Loss: 0.0788\n",
      "Epoch 90/200 — Train Loss: 0.0763 | Val Loss: 0.0775\n",
      "Epoch 91/200 — Train Loss: 0.0759 | Val Loss: 0.0770\n",
      "Epoch 92/200 — Train Loss: 0.0759 | Val Loss: 0.0785\n",
      "Epoch 93/200 — Train Loss: 0.0756 | Val Loss: 0.0783\n",
      "Epoch 94/200 — Train Loss: 0.0750 | Val Loss: 0.0780\n",
      "Epoch 95/200 — Train Loss: 0.0752 | Val Loss: 0.0768\n",
      "Epoch 96/200 — Train Loss: 0.0748 | Val Loss: 0.0783\n",
      "Epoch 97/200 — Train Loss: 0.0747 | Val Loss: 0.0780\n",
      "Epoch 98/200 — Train Loss: 0.0746 | Val Loss: 0.0766\n",
      "Epoch 99/200 — Train Loss: 0.0743 | Val Loss: 0.0765\n",
      "Epoch 100/200 — Train Loss: 0.0744 | Val Loss: 0.0775\n",
      "Epoch 101/200 — Train Loss: 0.0744 | Val Loss: 0.0783\n",
      "Epoch 102/200 — Train Loss: 0.0742 | Val Loss: 0.0770\n",
      "Epoch 103/200 — Train Loss: 0.0737 | Val Loss: 0.0769\n",
      "Epoch 104/200 — Train Loss: 0.0739 | Val Loss: 0.0756\n",
      "Epoch 105/200 — Train Loss: 0.0737 | Val Loss: 0.0757\n",
      "Epoch 106/200 — Train Loss: 0.0740 | Val Loss: 0.0758\n",
      "Epoch 107/200 — Train Loss: 0.0735 | Val Loss: 0.0766\n",
      "Epoch 108/200 — Train Loss: 0.0733 | Val Loss: 0.0756\n",
      "Epoch 109/200 — Train Loss: 0.0730 | Val Loss: 0.0749\n",
      "Epoch 110/200 — Train Loss: 0.0729 | Val Loss: 0.0762\n",
      "Epoch 111/200 — Train Loss: 0.0730 | Val Loss: 0.0741\n",
      "Epoch 112/200 — Train Loss: 0.0726 | Val Loss: 0.0752\n",
      "Epoch 113/200 — Train Loss: 0.0724 | Val Loss: 0.0754\n",
      "Epoch 114/200 — Train Loss: 0.0726 | Val Loss: 0.0769\n",
      "Epoch 115/200 — Train Loss: 0.0726 | Val Loss: 0.0774\n",
      "Epoch 116/200 — Train Loss: 0.0723 | Val Loss: 0.0739\n",
      "Epoch 117/200 — Train Loss: 0.0724 | Val Loss: 0.0738\n",
      "Epoch 118/200 — Train Loss: 0.0719 | Val Loss: 0.0752\n",
      "Epoch 119/200 — Train Loss: 0.0717 | Val Loss: 0.0740\n",
      "Epoch 120/200 — Train Loss: 0.0712 | Val Loss: 0.0736\n",
      "Epoch 121/200 — Train Loss: 0.0717 | Val Loss: 0.0743\n",
      "Epoch 122/200 — Train Loss: 0.0714 | Val Loss: 0.0733\n",
      "Epoch 123/200 — Train Loss: 0.0711 | Val Loss: 0.0738\n",
      "Epoch 124/200 — Train Loss: 0.0713 | Val Loss: 0.0737\n",
      "Epoch 125/200 — Train Loss: 0.0712 | Val Loss: 0.0739\n",
      "Epoch 126/200 — Train Loss: 0.0712 | Val Loss: 0.0735\n",
      "Epoch 127/200 — Train Loss: 0.0714 | Val Loss: 0.0730\n",
      "Epoch 128/200 — Train Loss: 0.0705 | Val Loss: 0.0725\n",
      "Epoch 129/200 — Train Loss: 0.0705 | Val Loss: 0.0724\n",
      "Epoch 130/200 — Train Loss: 0.0703 | Val Loss: 0.0714\n",
      "Epoch 131/200 — Train Loss: 0.0703 | Val Loss: 0.0739\n",
      "Epoch 132/200 — Train Loss: 0.0702 | Val Loss: 0.0725\n",
      "Epoch 133/200 — Train Loss: 0.0702 | Val Loss: 0.0724\n",
      "Epoch 134/200 — Train Loss: 0.0700 | Val Loss: 0.0722\n",
      "Epoch 135/200 — Train Loss: 0.0696 | Val Loss: 0.0725\n",
      "Epoch 136/200 — Train Loss: 0.0700 | Val Loss: 0.0724\n",
      "Epoch 137/200 — Train Loss: 0.0696 | Val Loss: 0.0715\n",
      "Epoch 138/200 — Train Loss: 0.0697 | Val Loss: 0.0710\n",
      "Epoch 139/200 — Train Loss: 0.0695 | Val Loss: 0.0709\n",
      "Epoch 140/200 — Train Loss: 0.0692 | Val Loss: 0.0712\n",
      "Epoch 141/200 — Train Loss: 0.0688 | Val Loss: 0.0712\n",
      "Epoch 142/200 — Train Loss: 0.0691 | Val Loss: 0.0702\n",
      "Epoch 143/200 — Train Loss: 0.0692 | Val Loss: 0.0705\n",
      "Epoch 144/200 — Train Loss: 0.0692 | Val Loss: 0.0701\n",
      "Epoch 145/200 — Train Loss: 0.0688 | Val Loss: 0.0718\n",
      "Epoch 146/200 — Train Loss: 0.0686 | Val Loss: 0.0704\n",
      "Epoch 147/200 — Train Loss: 0.0690 | Val Loss: 0.0713\n",
      "Epoch 148/200 — Train Loss: 0.0685 | Val Loss: 0.0697\n",
      "Epoch 149/200 — Train Loss: 0.0685 | Val Loss: 0.0697\n",
      "Epoch 150/200 — Train Loss: 0.0685 | Val Loss: 0.0694\n",
      "Epoch 151/200 — Train Loss: 0.0682 | Val Loss: 0.0714\n",
      "Epoch 152/200 — Train Loss: 0.0681 | Val Loss: 0.0718\n",
      "Epoch 153/200 — Train Loss: 0.0681 | Val Loss: 0.0742\n",
      "Epoch 154/200 — Train Loss: 0.0683 | Val Loss: 0.0692\n",
      "Epoch 155/200 — Train Loss: 0.0681 | Val Loss: 0.0696\n",
      "Epoch 156/200 — Train Loss: 0.0680 | Val Loss: 0.0699\n",
      "Epoch 157/200 — Train Loss: 0.0677 | Val Loss: 0.0691\n",
      "Epoch 158/200 — Train Loss: 0.0677 | Val Loss: 0.0693\n",
      "Epoch 159/200 — Train Loss: 0.0676 | Val Loss: 0.0705\n",
      "Epoch 160/200 — Train Loss: 0.0676 | Val Loss: 0.0697\n",
      "Epoch 161/200 — Train Loss: 0.0673 | Val Loss: 0.0696\n",
      "Epoch 162/200 — Train Loss: 0.0670 | Val Loss: 0.0691\n",
      "Epoch 163/200 — Train Loss: 0.0676 | Val Loss: 0.0695\n",
      "Epoch 164/200 — Train Loss: 0.0669 | Val Loss: 0.0681\n",
      "Epoch 165/200 — Train Loss: 0.0667 | Val Loss: 0.0714\n",
      "Epoch 166/200 — Train Loss: 0.0674 | Val Loss: 0.0682\n",
      "Epoch 167/200 — Train Loss: 0.0666 | Val Loss: 0.0692\n",
      "Epoch 168/200 — Train Loss: 0.0667 | Val Loss: 0.0690\n",
      "Epoch 169/200 — Train Loss: 0.0664 | Val Loss: 0.0687\n",
      "Epoch 170/200 — Train Loss: 0.0664 | Val Loss: 0.0672\n",
      "Epoch 171/200 — Train Loss: 0.0662 | Val Loss: 0.0676\n",
      "Epoch 172/200 — Train Loss: 0.0664 | Val Loss: 0.0680\n",
      "Epoch 173/200 — Train Loss: 0.0664 | Val Loss: 0.0687\n",
      "Epoch 174/200 — Train Loss: 0.0660 | Val Loss: 0.0673\n",
      "Epoch 175/200 — Train Loss: 0.0658 | Val Loss: 0.0677\n",
      "Epoch 176/200 — Train Loss: 0.0663 | Val Loss: 0.0679\n",
      "Epoch 177/200 — Train Loss: 0.0661 | Val Loss: 0.0686\n",
      "Epoch 178/200 — Train Loss: 0.0654 | Val Loss: 0.0669\n",
      "Epoch 179/200 — Train Loss: 0.0656 | Val Loss: 0.0667\n",
      "Epoch 180/200 — Train Loss: 0.0656 | Val Loss: 0.0678\n",
      "Epoch 181/200 — Train Loss: 0.0658 | Val Loss: 0.0671\n",
      "Epoch 182/200 — Train Loss: 0.0657 | Val Loss: 0.0673\n",
      "Epoch 183/200 — Train Loss: 0.0654 | Val Loss: 0.0674\n",
      "Epoch 184/200 — Train Loss: 0.0657 | Val Loss: 0.0672\n",
      "Epoch 185/200 — Train Loss: 0.0651 | Val Loss: 0.0666\n",
      "Epoch 186/200 — Train Loss: 0.0651 | Val Loss: 0.0667\n",
      "Epoch 187/200 — Train Loss: 0.0652 | Val Loss: 0.0679\n",
      "Epoch 188/200 — Train Loss: 0.0651 | Val Loss: 0.0662\n",
      "Epoch 189/200 — Train Loss: 0.0648 | Val Loss: 0.0672\n",
      "Epoch 190/200 — Train Loss: 0.0651 | Val Loss: 0.0680\n",
      "Epoch 191/200 — Train Loss: 0.0650 | Val Loss: 0.0680\n",
      "Epoch 192/200 — Train Loss: 0.0650 | Val Loss: 0.0662\n",
      "Epoch 193/200 — Train Loss: 0.0647 | Val Loss: 0.0667\n",
      "Epoch 194/200 — Train Loss: 0.0643 | Val Loss: 0.0669\n",
      "Epoch 195/200 — Train Loss: 0.0646 | Val Loss: 0.0676\n",
      "Epoch 196/200 — Train Loss: 0.0648 | Val Loss: 0.0673\n",
      "Epoch 197/200 — Train Loss: 0.0650 | Val Loss: 0.0667\n",
      "Epoch 198/200 — Train Loss: 0.0643 | Val Loss: 0.0661\n",
      "Epoch 199/200 — Train Loss: 0.0645 | Val Loss: 0.0676\n",
      "Epoch 200/200 — Train Loss: 0.0640 | Val Loss: 0.0662\n"
     ]
    }
   ],
   "source": [
    "# ─── 8) Define & train MLP on synthetic ────────────────────────────────────────\n",
    "class PresenceNet(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(D,128), nn.ReLU(),\n",
    "            nn.Linear(128,64), nn.ReLU(),\n",
    "            nn.Linear(64,C),   nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "D = syn_embeds.shape[1]\n",
    "model_p = PresenceNet(D, C)\n",
    "criterion = nn.BCELoss()\n",
    "opt = optim.Adam(model_p.parameters(), lr=1e-3)\n",
    "epochs = 200\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    model_p.train()\n",
    "    loss_tr = 0\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model_p(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        loss_tr += loss.item()*xb.size(0)\n",
    "    loss_tr /= len(train_loader.dataset)\n",
    "    model_p.eval()\n",
    "    loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            loss_val += criterion(model_p(xb), yb).item()*xb.size(0)\n",
    "    loss_val /= len(val_loader.dataset)\n",
    "    print(f\"Epoch {e}/{epochs} — Train Loss: {loss_tr:.4f} | Val Loss: {loss_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0645f62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synthetic Test Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "           1,9-nonanedithiol       0.87      0.86      0.86       196\n",
      "             1-dodecanethiol       0.75      0.74      0.74       225\n",
      "             1-undecanethiol       0.57      0.42      0.48       208\n",
      "        6-mercapto-1-hexanol       0.98      0.70      0.82       204\n",
      "                     benzene       1.00      1.00      1.00       235\n",
      "                benzenethiol       0.96      0.94      0.95       195\n",
      "                        dmmp       1.00      1.00      1.00       220\n",
      "                        etoh       0.97      0.91      0.94       217\n",
      "                        meoh       1.00      0.92      0.96       184\n",
      "       n,n-dimethylformamide       1.00      0.97      0.99       212\n",
      "                    pyridine       1.00      0.99      1.00       223\n",
      "tris(2-ethylhexyl) phosphate       0.97      0.98      0.97       189\n",
      "\n",
      "                   micro avg       0.93      0.87      0.90      2508\n",
      "                   macro avg       0.92      0.87      0.89      2508\n",
      "                weighted avg       0.92      0.87      0.89      2508\n",
      "                 samples avg       0.94      0.87      0.89      2508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 9) Evaluate synthetic test set ────────────────────────────────────────────\n",
    "model_p.eval()\n",
    "yp, yt = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        yp.append(model_p(xb).numpy())\n",
    "        yt.append(yb.numpy())\n",
    "y_pred = (np.vstack(yp)>0.5).astype(int)\n",
    "y_true = np.vstack(yt)\n",
    "print(\"\\nSynthetic Test Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3e05e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 338 tasks      | elapsed:    8.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixtures preprocessed: (580, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 580 out of 580 | elapsed:   13.4s finished\n"
     ]
    }
   ],
   "source": [
    "# --- Load mixtures and convert columns ---\n",
    "mix_df = pd.read_csv('mixtures_dataset.csv')\n",
    "\n",
    "# Re-use your floatify_cols helper:\n",
    "def floatify_cols(df):\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        if c in ('Label 1', 'Label 2'):\n",
    "            new_cols.append(c)\n",
    "        else:\n",
    "            new_cols.append(float(c))  # convert wavenumber strings → floats\n",
    "    df.columns = new_cols\n",
    "\n",
    "floatify_cols(mix_df)\n",
    "\n",
    "# --- Now select wavenumber columns (they are all numeric) ---\n",
    "wav_cols = [c for c in mix_df.columns if c not in ('Label 1', 'Label 2')]\n",
    "\n",
    "# --- Extract spectra as a pure float array ---\n",
    "mix_specs = mix_df[wav_cols].values.astype(float)  # ensure float64 dtype\n",
    "\n",
    "# --- Parallel preprocessing now works because mix_specs is numeric ---\n",
    "mix_proc = np.vstack(\n",
    "    Parallel(n_jobs=-1, verbose=5)(\n",
    "        delayed(preprocess_single)(spec) for spec in mix_specs\n",
    "    )\n",
    ")\n",
    "print(\"Mixtures preprocessed:\", mix_proc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a19c41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs     = list(zip(mix_df['Label 1'], mix_df['Label 2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42bc50f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real Mixtures Validation Report (labels with support > 0):\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "      1-dodecanethiol       1.00      0.44      0.62       243\n",
      " 6-mercapto-1-hexanol       0.90      0.33      0.49       108\n",
      "              benzene       1.00      1.00      1.00       193\n",
      "         benzenethiol       0.33      0.50      0.40        72\n",
      "                 etoh       1.00      0.70      0.83       121\n",
      "                 meoh       1.00      0.97      0.99       243\n",
      "n,n-dimethylformamide       1.00      1.00      1.00        72\n",
      "             pyridine       1.00      1.00      1.00       108\n",
      "\n",
      "            micro avg       0.92      0.75      0.83      1160\n",
      "            macro avg       0.90      0.74      0.79      1160\n",
      "         weighted avg       0.95      0.75      0.81      1160\n",
      "          samples avg       0.93      0.75      0.81      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mix_embeds = siamese(torch.tensor(mix_proc, dtype=torch.float32).unsqueeze(1)).cpu().numpy()\n",
    "\n",
    "# build real multi-hot\n",
    "N_real = len(mix_df)\n",
    "Y_real = np.zeros((N_real, C), dtype=int)\n",
    "for i, (l1, l2) in enumerate(pairs):\n",
    "    Y_real[i, class_to_i[l1]] = 1\n",
    "    Y_real[i, class_to_i[l2]] = 1\n",
    "\n",
    "# predict real\n",
    "model_p.eval()\n",
    "preds = model_p(torch.tensor(mix_embeds, dtype=torch.float32)).detach().numpy()\n",
    "Y_pred_real = (preds>0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1) Compute support for each class\n",
    "supports = Y_real.sum(axis=0)   # length C array of counts\n",
    "\n",
    "# 2) Select only the classes with support > 0\n",
    "valid_idx = [i for i, s in enumerate(supports) if s > 0]\n",
    "valid_labels = [classes[i] for i in valid_idx]\n",
    "\n",
    "# 3) Filter y_true and y_pred to these columns\n",
    "y_true_filt = Y_real[:, valid_idx]\n",
    "y_pred_filt = Y_pred_real[:, valid_idx]\n",
    "\n",
    "# 4) Print report on the filtered set\n",
    "print(\"\\nReal Mixtures Validation Report (labels with support > 0):\")\n",
    "print(classification_report(\n",
    "    y_true_filt,\n",
    "    y_pred_filt,\n",
    "    target_names=valid_labels,\n",
    "    zero_division=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07ffc8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define the PresenceNet WITHOUT final Sigmoid\n",
    "class PresenceNetLogits(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(D, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, C)   # raw logits\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f8862bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Instantiate model\n",
    "D = X_train_s.shape[1]  # embedding dimension\n",
    "C = len(classes)\n",
    "model_boost = PresenceNetLogits(D, C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07440583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight: tensor([4.9714, 5.0726, 5.0216, 4.9928, 5.0800, 4.9326, 5.1096, 5.0000, 4.9361,\n",
      "        4.9750, 5.0144, 4.9012])\n"
     ]
    }
   ],
   "source": [
    "# 3) Build pos_weight to up-weight specific classes\n",
    "\n",
    "pos = Y_train_s.sum(axis=0)\n",
    "neg = len(Y_train_s) - pos\n",
    "pos_weight = torch.tensor((neg/pos).clip(min=1.0), dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(\"pos_weight:\", pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81c01c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 — Train Loss: 0.9226 | Val Loss: 0.6375\n",
      "Epoch 2/200 — Train Loss: 0.5625 | Val Loss: 0.5154\n",
      "Epoch 3/200 — Train Loss: 0.4966 | Val Loss: 0.4706\n",
      "Epoch 4/200 — Train Loss: 0.4548 | Val Loss: 0.4323\n",
      "Epoch 5/200 — Train Loss: 0.4206 | Val Loss: 0.4058\n",
      "Epoch 6/200 — Train Loss: 0.3944 | Val Loss: 0.3803\n",
      "Epoch 7/200 — Train Loss: 0.3748 | Val Loss: 0.3639\n",
      "Epoch 8/200 — Train Loss: 0.3580 | Val Loss: 0.3549\n",
      "Epoch 9/200 — Train Loss: 0.3453 | Val Loss: 0.3412\n",
      "Epoch 10/200 — Train Loss: 0.3342 | Val Loss: 0.3331\n",
      "Epoch 11/200 — Train Loss: 0.3245 | Val Loss: 0.3191\n",
      "Epoch 12/200 — Train Loss: 0.3135 | Val Loss: 0.3098\n",
      "Epoch 13/200 — Train Loss: 0.3057 | Val Loss: 0.3043\n",
      "Epoch 14/200 — Train Loss: 0.2972 | Val Loss: 0.2978\n",
      "Epoch 15/200 — Train Loss: 0.2897 | Val Loss: 0.2889\n",
      "Epoch 16/200 — Train Loss: 0.2818 | Val Loss: 0.2816\n",
      "Epoch 17/200 — Train Loss: 0.2745 | Val Loss: 0.2726\n",
      "Epoch 18/200 — Train Loss: 0.2694 | Val Loss: 0.2664\n",
      "Epoch 19/200 — Train Loss: 0.2631 | Val Loss: 0.2628\n",
      "Epoch 20/200 — Train Loss: 0.2580 | Val Loss: 0.2545\n",
      "Epoch 21/200 — Train Loss: 0.2528 | Val Loss: 0.2534\n",
      "Epoch 22/200 — Train Loss: 0.2483 | Val Loss: 0.2475\n",
      "Epoch 23/200 — Train Loss: 0.2441 | Val Loss: 0.2416\n",
      "Epoch 24/200 — Train Loss: 0.2394 | Val Loss: 0.2408\n",
      "Epoch 25/200 — Train Loss: 0.2363 | Val Loss: 0.2346\n",
      "Epoch 26/200 — Train Loss: 0.2324 | Val Loss: 0.2312\n",
      "Epoch 27/200 — Train Loss: 0.2297 | Val Loss: 0.2270\n",
      "Epoch 28/200 — Train Loss: 0.2267 | Val Loss: 0.2239\n",
      "Epoch 29/200 — Train Loss: 0.2233 | Val Loss: 0.2256\n",
      "Epoch 30/200 — Train Loss: 0.2207 | Val Loss: 0.2221\n",
      "Epoch 31/200 — Train Loss: 0.2179 | Val Loss: 0.2178\n",
      "Epoch 32/200 — Train Loss: 0.2154 | Val Loss: 0.2192\n",
      "Epoch 33/200 — Train Loss: 0.2131 | Val Loss: 0.2136\n",
      "Epoch 34/200 — Train Loss: 0.2110 | Val Loss: 0.2100\n",
      "Epoch 35/200 — Train Loss: 0.2093 | Val Loss: 0.2093\n",
      "Epoch 36/200 — Train Loss: 0.2071 | Val Loss: 0.2104\n",
      "Epoch 37/200 — Train Loss: 0.2056 | Val Loss: 0.2041\n",
      "Epoch 38/200 — Train Loss: 0.2032 | Val Loss: 0.2052\n",
      "Epoch 39/200 — Train Loss: 0.2032 | Val Loss: 0.2076\n",
      "Epoch 40/200 — Train Loss: 0.2000 | Val Loss: 0.2003\n",
      "Epoch 41/200 — Train Loss: 0.1991 | Val Loss: 0.2032\n",
      "Epoch 42/200 — Train Loss: 0.1972 | Val Loss: 0.2002\n",
      "Epoch 43/200 — Train Loss: 0.1963 | Val Loss: 0.1994\n",
      "Epoch 44/200 — Train Loss: 0.1954 | Val Loss: 0.1943\n",
      "Epoch 45/200 — Train Loss: 0.1933 | Val Loss: 0.1986\n",
      "Epoch 46/200 — Train Loss: 0.1918 | Val Loss: 0.1975\n",
      "Epoch 47/200 — Train Loss: 0.1913 | Val Loss: 0.1982\n",
      "Epoch 48/200 — Train Loss: 0.1904 | Val Loss: 0.1942\n",
      "Epoch 49/200 — Train Loss: 0.1884 | Val Loss: 0.1906\n",
      "Epoch 50/200 — Train Loss: 0.1881 | Val Loss: 0.1904\n",
      "Epoch 51/200 — Train Loss: 0.1880 | Val Loss: 0.1917\n",
      "Epoch 52/200 — Train Loss: 0.1857 | Val Loss: 0.1905\n",
      "Epoch 53/200 — Train Loss: 0.1849 | Val Loss: 0.1912\n",
      "Epoch 54/200 — Train Loss: 0.1844 | Val Loss: 0.1883\n",
      "Epoch 55/200 — Train Loss: 0.1839 | Val Loss: 0.1869\n",
      "Epoch 56/200 — Train Loss: 0.1822 | Val Loss: 0.1856\n",
      "Epoch 57/200 — Train Loss: 0.1821 | Val Loss: 0.1844\n",
      "Epoch 58/200 — Train Loss: 0.1807 | Val Loss: 0.1833\n",
      "Epoch 59/200 — Train Loss: 0.1806 | Val Loss: 0.1818\n",
      "Epoch 60/200 — Train Loss: 0.1793 | Val Loss: 0.1846\n",
      "Epoch 61/200 — Train Loss: 0.1788 | Val Loss: 0.1833\n",
      "Epoch 62/200 — Train Loss: 0.1783 | Val Loss: 0.1827\n",
      "Epoch 63/200 — Train Loss: 0.1773 | Val Loss: 0.1820\n",
      "Epoch 64/200 — Train Loss: 0.1768 | Val Loss: 0.1794\n",
      "Epoch 65/200 — Train Loss: 0.1770 | Val Loss: 0.1780\n",
      "Epoch 66/200 — Train Loss: 0.1757 | Val Loss: 0.1784\n",
      "Epoch 67/200 — Train Loss: 0.1749 | Val Loss: 0.1770\n",
      "Epoch 68/200 — Train Loss: 0.1745 | Val Loss: 0.1773\n",
      "Epoch 69/200 — Train Loss: 0.1739 | Val Loss: 0.1767\n",
      "Epoch 70/200 — Train Loss: 0.1733 | Val Loss: 0.1771\n",
      "Epoch 71/200 — Train Loss: 0.1726 | Val Loss: 0.1782\n",
      "Epoch 72/200 — Train Loss: 0.1726 | Val Loss: 0.1742\n",
      "Epoch 73/200 — Train Loss: 0.1725 | Val Loss: 0.1770\n",
      "Epoch 74/200 — Train Loss: 0.1709 | Val Loss: 0.1785\n",
      "Epoch 75/200 — Train Loss: 0.1707 | Val Loss: 0.1721\n",
      "Epoch 76/200 — Train Loss: 0.1704 | Val Loss: 0.1757\n",
      "Epoch 77/200 — Train Loss: 0.1691 | Val Loss: 0.1729\n",
      "Epoch 78/200 — Train Loss: 0.1688 | Val Loss: 0.1743\n",
      "Epoch 79/200 — Train Loss: 0.1681 | Val Loss: 0.1781\n",
      "Epoch 80/200 — Train Loss: 0.1683 | Val Loss: 0.1801\n",
      "Epoch 81/200 — Train Loss: 0.1681 | Val Loss: 0.1716\n",
      "Epoch 82/200 — Train Loss: 0.1667 | Val Loss: 0.1716\n",
      "Epoch 83/200 — Train Loss: 0.1664 | Val Loss: 0.1692\n",
      "Epoch 84/200 — Train Loss: 0.1658 | Val Loss: 0.1736\n",
      "Epoch 85/200 — Train Loss: 0.1666 | Val Loss: 0.1698\n",
      "Epoch 86/200 — Train Loss: 0.1650 | Val Loss: 0.1710\n",
      "Epoch 87/200 — Train Loss: 0.1667 | Val Loss: 0.1742\n",
      "Epoch 88/200 — Train Loss: 0.1649 | Val Loss: 0.1755\n",
      "Epoch 89/200 — Train Loss: 0.1640 | Val Loss: 0.1704\n",
      "Epoch 90/200 — Train Loss: 0.1632 | Val Loss: 0.1717\n",
      "Epoch 91/200 — Train Loss: 0.1633 | Val Loss: 0.1715\n",
      "Epoch 92/200 — Train Loss: 0.1634 | Val Loss: 0.1688\n",
      "Epoch 93/200 — Train Loss: 0.1630 | Val Loss: 0.1668\n",
      "Epoch 94/200 — Train Loss: 0.1622 | Val Loss: 0.1692\n",
      "Epoch 95/200 — Train Loss: 0.1621 | Val Loss: 0.1679\n",
      "Epoch 96/200 — Train Loss: 0.1611 | Val Loss: 0.1634\n",
      "Epoch 97/200 — Train Loss: 0.1609 | Val Loss: 0.1676\n",
      "Epoch 98/200 — Train Loss: 0.1615 | Val Loss: 0.1675\n",
      "Epoch 99/200 — Train Loss: 0.1608 | Val Loss: 0.1655\n",
      "Epoch 100/200 — Train Loss: 0.1605 | Val Loss: 0.1655\n",
      "Epoch 101/200 — Train Loss: 0.1594 | Val Loss: 0.1648\n",
      "Epoch 102/200 — Train Loss: 0.1601 | Val Loss: 0.1671\n",
      "Epoch 103/200 — Train Loss: 0.1589 | Val Loss: 0.1662\n",
      "Epoch 104/200 — Train Loss: 0.1590 | Val Loss: 0.1630\n",
      "Epoch 105/200 — Train Loss: 0.1591 | Val Loss: 0.1698\n",
      "Epoch 106/200 — Train Loss: 0.1588 | Val Loss: 0.1617\n",
      "Epoch 107/200 — Train Loss: 0.1579 | Val Loss: 0.1628\n",
      "Epoch 108/200 — Train Loss: 0.1576 | Val Loss: 0.1654\n",
      "Epoch 109/200 — Train Loss: 0.1567 | Val Loss: 0.1626\n",
      "Epoch 110/200 — Train Loss: 0.1570 | Val Loss: 0.1672\n",
      "Epoch 111/200 — Train Loss: 0.1579 | Val Loss: 0.1634\n",
      "Epoch 112/200 — Train Loss: 0.1568 | Val Loss: 0.1634\n",
      "Epoch 113/200 — Train Loss: 0.1554 | Val Loss: 0.1635\n",
      "Epoch 114/200 — Train Loss: 0.1548 | Val Loss: 0.1650\n",
      "Epoch 115/200 — Train Loss: 0.1563 | Val Loss: 0.1625\n",
      "Epoch 116/200 — Train Loss: 0.1556 | Val Loss: 0.1626\n",
      "Epoch 117/200 — Train Loss: 0.1551 | Val Loss: 0.1614\n",
      "Epoch 118/200 — Train Loss: 0.1554 | Val Loss: 0.1611\n",
      "Epoch 119/200 — Train Loss: 0.1549 | Val Loss: 0.1653\n",
      "Epoch 120/200 — Train Loss: 0.1546 | Val Loss: 0.1603\n",
      "Epoch 121/200 — Train Loss: 0.1539 | Val Loss: 0.1616\n",
      "Epoch 122/200 — Train Loss: 0.1547 | Val Loss: 0.1600\n",
      "Epoch 123/200 — Train Loss: 0.1539 | Val Loss: 0.1587\n",
      "Epoch 124/200 — Train Loss: 0.1536 | Val Loss: 0.1568\n",
      "Epoch 125/200 — Train Loss: 0.1535 | Val Loss: 0.1575\n",
      "Epoch 126/200 — Train Loss: 0.1522 | Val Loss: 0.1559\n",
      "Epoch 127/200 — Train Loss: 0.1519 | Val Loss: 0.1572\n",
      "Epoch 128/200 — Train Loss: 0.1526 | Val Loss: 0.1574\n",
      "Epoch 129/200 — Train Loss: 0.1533 | Val Loss: 0.1560\n",
      "Epoch 130/200 — Train Loss: 0.1526 | Val Loss: 0.1585\n",
      "Epoch 131/200 — Train Loss: 0.1515 | Val Loss: 0.1547\n",
      "Epoch 132/200 — Train Loss: 0.1517 | Val Loss: 0.1577\n",
      "Epoch 133/200 — Train Loss: 0.1508 | Val Loss: 0.1587\n",
      "Epoch 134/200 — Train Loss: 0.1516 | Val Loss: 0.1565\n",
      "Epoch 135/200 — Train Loss: 0.1504 | Val Loss: 0.1602\n",
      "Epoch 136/200 — Train Loss: 0.1503 | Val Loss: 0.1540\n",
      "Epoch 137/200 — Train Loss: 0.1512 | Val Loss: 0.1562\n",
      "Epoch 138/200 — Train Loss: 0.1497 | Val Loss: 0.1574\n",
      "Epoch 139/200 — Train Loss: 0.1498 | Val Loss: 0.1545\n",
      "Epoch 140/200 — Train Loss: 0.1492 | Val Loss: 0.1552\n",
      "Epoch 141/200 — Train Loss: 0.1501 | Val Loss: 0.1530\n",
      "Epoch 142/200 — Train Loss: 0.1489 | Val Loss: 0.1617\n",
      "Epoch 143/200 — Train Loss: 0.1492 | Val Loss: 0.1536\n",
      "Epoch 144/200 — Train Loss: 0.1489 | Val Loss: 0.1550\n",
      "Epoch 145/200 — Train Loss: 0.1477 | Val Loss: 0.1525\n",
      "Epoch 146/200 — Train Loss: 0.1490 | Val Loss: 0.1564\n",
      "Epoch 147/200 — Train Loss: 0.1477 | Val Loss: 0.1514\n",
      "Epoch 148/200 — Train Loss: 0.1477 | Val Loss: 0.1507\n",
      "Epoch 149/200 — Train Loss: 0.1479 | Val Loss: 0.1539\n",
      "Epoch 150/200 — Train Loss: 0.1471 | Val Loss: 0.1503\n",
      "Epoch 151/200 — Train Loss: 0.1464 | Val Loss: 0.1511\n",
      "Epoch 152/200 — Train Loss: 0.1465 | Val Loss: 0.1525\n",
      "Epoch 153/200 — Train Loss: 0.1463 | Val Loss: 0.1495\n",
      "Epoch 154/200 — Train Loss: 0.1461 | Val Loss: 0.1524\n",
      "Epoch 155/200 — Train Loss: 0.1458 | Val Loss: 0.1587\n",
      "Epoch 156/200 — Train Loss: 0.1459 | Val Loss: 0.1536\n",
      "Epoch 157/200 — Train Loss: 0.1463 | Val Loss: 0.1519\n",
      "Epoch 158/200 — Train Loss: 0.1452 | Val Loss: 0.1492\n",
      "Epoch 159/200 — Train Loss: 0.1454 | Val Loss: 0.1543\n",
      "Epoch 160/200 — Train Loss: 0.1442 | Val Loss: 0.1495\n",
      "Epoch 161/200 — Train Loss: 0.1447 | Val Loss: 0.1471\n",
      "Epoch 162/200 — Train Loss: 0.1436 | Val Loss: 0.1478\n",
      "Epoch 163/200 — Train Loss: 0.1438 | Val Loss: 0.1477\n",
      "Epoch 164/200 — Train Loss: 0.1435 | Val Loss: 0.1503\n",
      "Epoch 165/200 — Train Loss: 0.1434 | Val Loss: 0.1460\n",
      "Epoch 166/200 — Train Loss: 0.1439 | Val Loss: 0.1499\n",
      "Epoch 167/200 — Train Loss: 0.1438 | Val Loss: 0.1506\n",
      "Epoch 168/200 — Train Loss: 0.1435 | Val Loss: 0.1485\n",
      "Epoch 169/200 — Train Loss: 0.1433 | Val Loss: 0.1483\n",
      "Epoch 170/200 — Train Loss: 0.1428 | Val Loss: 0.1505\n",
      "Epoch 171/200 — Train Loss: 0.1434 | Val Loss: 0.1478\n",
      "Epoch 172/200 — Train Loss: 0.1424 | Val Loss: 0.1475\n",
      "Epoch 173/200 — Train Loss: 0.1421 | Val Loss: 0.1482\n",
      "Epoch 174/200 — Train Loss: 0.1414 | Val Loss: 0.1482\n",
      "Epoch 175/200 — Train Loss: 0.1422 | Val Loss: 0.1506\n",
      "Epoch 176/200 — Train Loss: 0.1417 | Val Loss: 0.1480\n",
      "Epoch 177/200 — Train Loss: 0.1417 | Val Loss: 0.1452\n",
      "Epoch 178/200 — Train Loss: 0.1413 | Val Loss: 0.1468\n",
      "Epoch 179/200 — Train Loss: 0.1417 | Val Loss: 0.1434\n",
      "Epoch 180/200 — Train Loss: 0.1401 | Val Loss: 0.1497\n",
      "Epoch 181/200 — Train Loss: 0.1410 | Val Loss: 0.1475\n",
      "Epoch 182/200 — Train Loss: 0.1406 | Val Loss: 0.1451\n",
      "Epoch 183/200 — Train Loss: 0.1400 | Val Loss: 0.1454\n",
      "Epoch 184/200 — Train Loss: 0.1402 | Val Loss: 0.1457\n",
      "Epoch 185/200 — Train Loss: 0.1407 | Val Loss: 0.1464\n",
      "Epoch 186/200 — Train Loss: 0.1405 | Val Loss: 0.1436\n",
      "Epoch 187/200 — Train Loss: 0.1396 | Val Loss: 0.1441\n",
      "Epoch 188/200 — Train Loss: 0.1396 | Val Loss: 0.1417\n",
      "Epoch 189/200 — Train Loss: 0.1389 | Val Loss: 0.1486\n",
      "Epoch 190/200 — Train Loss: 0.1385 | Val Loss: 0.1468\n",
      "Epoch 191/200 — Train Loss: 0.1389 | Val Loss: 0.1445\n",
      "Epoch 192/200 — Train Loss: 0.1397 | Val Loss: 0.1399\n",
      "Epoch 193/200 — Train Loss: 0.1387 | Val Loss: 0.1414\n",
      "Epoch 194/200 — Train Loss: 0.1378 | Val Loss: 0.1431\n",
      "Epoch 195/200 — Train Loss: 0.1375 | Val Loss: 0.1440\n",
      "Epoch 196/200 — Train Loss: 0.1380 | Val Loss: 0.1463\n",
      "Epoch 197/200 — Train Loss: 0.1383 | Val Loss: 0.1439\n",
      "Epoch 198/200 — Train Loss: 0.1375 | Val Loss: 0.1435\n",
      "Epoch 199/200 — Train Loss: 0.1378 | Val Loss: 0.1448\n",
      "Epoch 200/200 — Train Loss: 0.1372 | Val Loss: 0.1388\n"
     ]
    }
   ],
   "source": [
    "# 4) Use BCEWithLogitsLoss with pos_weight\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model_boost.parameters(), lr=1e-3)\n",
    "\n",
    "# 5) Training loop skeleton\n",
    "num_epochs = 200\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model_boost.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        logits = model_boost(xb)\n",
    "        loss = criterion(logits, yb)  # yb must be FloatTensor\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation...\n",
    "    model_boost.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            logits = model_boost(xb)\n",
    "            val_loss += criterion(logits, yb).item() * xb.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs} — \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f84ffef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real Mixtures Validation Report (labels with support > 0):\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "      1-dodecanethiol       0.84      0.77      0.80       243\n",
      " 6-mercapto-1-hexanol       0.88      0.33      0.48       108\n",
      "              benzene       1.00      1.00      1.00       193\n",
      "         benzenethiol       0.67      1.00      0.80        72\n",
      "                 etoh       1.00      1.00      1.00       121\n",
      "                 meoh       1.00      0.96      0.98       243\n",
      "n,n-dimethylformamide       1.00      1.00      1.00        72\n",
      "             pyridine       1.00      1.00      1.00       108\n",
      "\n",
      "            micro avg       0.93      0.88      0.91      1160\n",
      "            macro avg       0.92      0.88      0.88      1160\n",
      "         weighted avg       0.93      0.88      0.89      1160\n",
      "          samples avg       0.94      0.88      0.89      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mix_embeds = siamese(torch.tensor(mix_proc, dtype=torch.float32).unsqueeze(1)).cpu().numpy()\n",
    "\n",
    "# build real multi-hot\n",
    "N_real = len(mix_df)\n",
    "Y_real = np.zeros((N_real, C), dtype=int)\n",
    "for i, (l1, l2) in enumerate(pairs):\n",
    "    Y_real[i, class_to_i[l1]] = 1\n",
    "    Y_real[i, class_to_i[l2]] = 1\n",
    "\n",
    "# predict real\n",
    "model_boost.eval()\n",
    "preds = model_boost(torch.tensor(mix_embeds, dtype=torch.float32)).detach().numpy()\n",
    "Y_pred_real = (preds>0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1) Compute support for each class\n",
    "supports = Y_real.sum(axis=0)   # length C array of counts\n",
    "\n",
    "# 2) Select only the classes with support > 0\n",
    "valid_idx = [i for i, s in enumerate(supports) if s > 0]\n",
    "valid_labels = [classes[i] for i in valid_idx]\n",
    "\n",
    "# 3) Filter y_true and y_pred to these columns\n",
    "y_true_filt = Y_real[:, valid_idx]\n",
    "y_pred_filt = Y_pred_real[:, valid_idx]\n",
    "\n",
    "# 4) Print report on the filtered set\n",
    "print(\"\\nReal Mixtures Validation Report (labels with support > 0):\")\n",
    "print(classification_report(\n",
    "    y_true_filt,\n",
    "    y_pred_filt,\n",
    "    target_names=valid_labels,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe0bb085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synthetic Test Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "           1,9-nonanedithiol       0.86      0.90      0.88       196\n",
      "             1-dodecanethiol       0.57      0.88      0.69       225\n",
      "             1-undecanethiol       0.48      0.88      0.62       208\n",
      "        6-mercapto-1-hexanol       0.81      0.92      0.86       204\n",
      "                     benzene       1.00      1.00      1.00       235\n",
      "                benzenethiol       0.93      0.99      0.96       195\n",
      "                        dmmp       1.00      1.00      1.00       220\n",
      "                        etoh       0.89      0.98      0.93       217\n",
      "                        meoh       0.96      0.95      0.95       184\n",
      "       n,n-dimethylformamide       1.00      0.98      0.99       212\n",
      "                    pyridine       1.00      0.99      1.00       223\n",
      "tris(2-ethylhexyl) phosphate       0.95      0.99      0.97       189\n",
      "\n",
      "                   micro avg       0.83      0.96      0.89      2508\n",
      "                   macro avg       0.87      0.96      0.91      2508\n",
      "                weighted avg       0.87      0.96      0.90      2508\n",
      "                 samples avg       0.87      0.96      0.90      2508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 9) Evaluate synthetic test set ────────────────────────────────────────────\n",
    "model_boost.eval()\n",
    "yp, yt = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        yp.append(model_boost(xb).numpy())\n",
    "        yt.append(yb.numpy())\n",
    "y_pred = (np.vstack(yp)>0.5).astype(int)\n",
    "y_true = np.vstack(yt)\n",
    "print(\"\\nSynthetic Test Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
